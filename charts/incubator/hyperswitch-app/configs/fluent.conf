@include kubernetes.conf

# Hyperswitch Server Source

<source>
  @type tail
  @id in_tail_hyperswitch-server-router_logs

  path /var/log/containers/hyperswitch-server*.log
  pos_file /var/log/fluentd-hyperswitch-server-router-containers.log.pos
  tag "hyperswitchserverrouterv3.*"
  read_from_head true
  <parse>
    @type regexp
    expression /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/
  </parse>
</source>

# Hyperswitch Consumer Source

<source>
  @type tail
  @id in_tail_hyperswitch-consumer_logs

  path /var/log/containers/hyperswitch-consumer*hyperswitch-*.log
  pos_file /var/log/fluentd-hyperswitch-consumer-containers.log.pos
  tag "consumer.*"
  read_from_head true
  <parse>
    @type regexp
    expression /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/
  </parse>
</source>


# HyperSwitch Producer Source

<source>
  @type tail
  @id in_tail_hyperswitch-producer_logs

  path /var/log/containers/hyperswitch-producer*hyperswitch-*.log
  pos_file /var/log/fluentd-hyperswitch-producer-containers.log.pos
  tag "producer.*"
  read_from_head true
  <parse>
    @type regexp
    expression /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/
  </parse>
</source>


# Hyperswitch Server Filter

<filter hyperswitchserverrouterv3.**>
  @type parser

  key_name log
  reserve_time true
  <parse>
    @type multi_format
    <pattern>
      format json
      hash_value_field json_log
      format_name 'json'
    </pattern>
    <pattern>
      format regexp
      expression /^(?<message>.*)$/
      format_name 'raw_message'
    </pattern>
  </parse>
</filter>

<filter hyperswitchserverrouterv3.**>
  @type kubernetes_metadata
</filter>

<filter hyperswitchserverrouterv3.**>
  @type grep
  <regexp>
    key $.kubernetes.container_name
    pattern /bach/
  </regexp>
</filter>

# HyperSwitch Consumer Filter

<filter consumer.**>
  @type parser

  key_name log
  reserve_time true
  <parse>
    @type multi_format
    <pattern>
      format json
      hash_value_field json_log
      format_name 'json'
    </pattern>
    <pattern>
      format regexp
      expression /^(?<message>.*)$/
      format_name 'raw_message'
    </pattern>
  </parse>
</filter>

<filter consumer.**>
  @type kubernetes_metadata
</filter>

<filter consumer.**>
  @type grep
  <regexp>
    key $.kubernetes.container_name
    pattern /consumer/
  </regexp>
</filter>    

# Hyperswicth Producer filter

<filter producer.**>
  @type parser

  key_name log
  reserve_time true
  <parse>
    @type multi_format
    <pattern>
      format json
      hash_value_field json_log
      format_name 'json'
    </pattern>
    <pattern>
      format regexp
      expression /^(?<message>.*)$/
      format_name 'raw_message'
    </pattern>
  </parse>
</filter>

<filter producer.**>
  @type kubernetes_metadata
</filter>

<filter producer.**>
  @type grep
  <regexp>
    key $.kubernetes.container_name
    pattern /producer/
  </regexp>
</filter>

# Hyperswitch Server Match  

<match hyperswitchserverrouterv3.**>
    <format>
      @type json
    </format>
    @type copy
      <store>
        @type opensearch
        @id hyperswitch-server-router_out_es
        id_key _hash
        remove_keys _hash
        @log_level debug
        include_tag_key true
        prefer_oj_serializer true
        reload_on_failure true
        reload_connections false
        request_timeout 120s
        bulk_message_request_threshold 10MB

        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
        ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
        logstash_prefix "hyperswitchserverrouterv3"
        logstash_format true
        type_name fluentd
        <buffer>
          @type file
          path /var/log/kafka-buffers/hyperswitch-server-router-buffer
          flush_thread_count 6
          flush_interval 1s
          chunk_limit_size 5M
          queue_limit_length 4
          flush_mode interval
          retry_max_interval 30
          retry_type exponential_backoff
          overflow_action drop_oldest_chunk
        </buffer>
    </store>
</match>

# Hyperswitch Consumer Match

<match consumer.**>
    <format>
      @type json
    </format>
    @type copy
      <store>
        @type opensearch
        @id hyperswitch-consumer_out_es
        id_key _hash
        remove_keys _hash
        @log_level debug
        include_tag_key true
        prefer_oj_serializer true
        reload_on_failure true
        reload_connections false
        request_timeout 120s
        bulk_message_request_threshold 10MB
        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
        ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
        logstash_prefix "consumer"
        logstash_format true
        type_name fluentd
        <buffer>
          @type file
          path /var/log/kafka-buffers/hyperswitch-consumer-buffer
          flush_thread_count 6
          flush_interval 1s
          chunk_limit_size 5M
          queue_limit_length 4
          flush_mode interval
          retry_max_interval 30
          retry_type exponential_backoff
          overflow_action drop_oldest_chunk
        </buffer>
    </store>
</match>   

# Hyperswitch Producer Match 
<match producer.**>
    <format>
      @type json
    </format>
    @type copy
      <store>
        @type opensearch
        @id hyperswitch-producer_out_es
        id_key _hash
        remove_keys _hash
        @log_level debug
        include_tag_key true
        prefer_oj_serializer true
        reload_on_failure true
        reload_connections false
        request_timeout 120s
        bulk_message_request_threshold 10MB
        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
        ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
        logstash_prefix "producer"
        logstash_format true
        type_name fluentd
        <buffer>
          @type file
          path /var/log/kafka-buffers/hyperswitch-producer-buffer
          flush_thread_count 6
          flush_interval 1s
          chunk_limit_size 5M
          queue_limit_length 4
          flush_mode interval
          retry_max_interval 30
          retry_type exponential_backoff
          overflow_action drop_oldest_chunk
        </buffer>
    </store>
</match>