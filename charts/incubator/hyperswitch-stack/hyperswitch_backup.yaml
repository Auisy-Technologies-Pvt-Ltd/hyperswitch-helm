apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 93ab53427cc68197b370234298563c92471a283e62e59d6048000a87cd546bd1
      checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      checksum/config-users-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    creationTimestamp: "2025-01-24T06:28:36Z"
    generateName: clickhouse-shard0-
    labels:
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/version: 24.10.2
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: clickhouse-shard0-8459b67cd6
      helm.sh/chart: clickhouse-6.3.3
      shard: "0"
      statefulset.kubernetes.io/pod-name: clickhouse-shard0-0
    name: clickhouse-shard0-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: clickhouse-shard0
      uid: e2a72ffa-ba4d-46c6-9a9d-221b5bca0d5e
    resourceVersion: "12739"
    uid: e5af6cc2-d6b1-4093-9ca7-be55cd8b7b08
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: clickhouse
                app.kubernetes.io/instance: hyperswitch-v1
                app.kubernetes.io/name: clickhouse
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - command:
      - /scripts/setup.sh
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: CLICKHOUSE_HTTP_PORT
        value: "8123"
      - name: CLICKHOUSE_TCP_PORT
        value: "9000"
      - name: CLICKHOUSE_MYSQL_PORT
        value: "9004"
      - name: CLICKHOUSE_POSTGRESQL_PORT
        value: "9005"
      - name: CLICKHOUSE_INTERSERVER_HTTP_PORT
        value: "9009"
      - name: CLICKHOUSE_ADMIN_USER
        value: default
      - name: CLICKHOUSE_SHARD_ID
        value: shard0
      - name: CLICKHOUSE_REPLICA_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: CLICKHOUSE_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: clickhouse
      - name: KEEPER_NODE_0
        value: hyperswitch-v1-zookeeper-0.hyperswitch-v1-zookeeper-headless.hyperswitch.svc.cluster.local
      image: docker.io/bitnami/clickhouse:24.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: http
        timeoutSeconds: 1
      name: clickhouse
      ports:
      - containerPort: 8123
        name: http
        protocol: TCP
      - containerPort: 9000
        name: tcp
        protocol: TCP
      - containerPort: 9005
        name: tcp-postgresql
        protocol: TCP
      - containerPort: 9004
        name: tcp-mysql
        protocol: TCP
      - containerPort: 9009
        name: http-intersrv
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/clickhouse/etc
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /opt/bitnami/clickhouse/logs
        name: empty-dir
        subPath: app-logs-dir
      - mountPath: /opt/bitnami/clickhouse/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /scripts/setup.sh
        name: scripts
        subPath: setup.sh
      - mountPath: /bitnami/clickhouse
        name: data
      - mountPath: /bitnami/clickhouse/etc/conf.d/default
        name: config
      - mountPath: /docker-entrypoint-initdb.d
        name: initdb-scripts
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: clickhouse-shard0-0
    initContainers:
    - command:
      - sh
      - -c
      - git clone --depth 1 --branch 2024.12.19.0 https://github.com/juspay/hyperswitch.git
        /scripts && cp /scripts/crates/analytics/docs/clickhouse/scripts/*.sql /docker-entrypoint-initdb.d/
      image: alpine/git
      imagePullPolicy: Always
      name: clone-sql-scripts
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /docker-entrypoint-initdb.d
        name: initdb-scripts
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: clickhouse
    serviceAccountName: clickhouse
    subdomain: clickhouse-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-clickhouse-shard0-0
    - configMap:
        defaultMode: 493
        name: clickhouse-scripts
      name: scripts
    - emptyDir: {}
      name: empty-dir
    - configMap:
        defaultMode: 420
        name: clickhouse
      name: config
    - emptyDir: {}
      name: initdb-scripts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1672cf15bf794f8ca1dd7620516f439b6210f4ed3e5c015a7c048ca2ac6a3181
      image: bitnami/clickhouse:24.3
      imageID: docker-pullable://bitnami/clickhouse@sha256:4e97b9f1f83713777eebe100d2569afcdc17203926e675d9110da3b003346bee
      lastState: {}
      name: clickhouse
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:43Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    initContainerStatuses:
    - containerID: docker://83d4f226bb09ae5554b00b627370dbee5e854302477217bcbbb3ab4928c472f4
      image: alpine/git:latest
      imageID: docker-pullable://alpine/git@sha256:7d136d615a2674e289a5b6510717c0dd1160af5e708510d144f25b53f83a71db
      lastState: {}
      name: clone-sql-scripts
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://83d4f226bb09ae5554b00b627370dbee5e854302477217bcbbb3ab4928c472f4
          exitCode: 0
          finishedAt: "2025-01-24T06:28:42Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:39Z"
    phase: Running
    podIP: 192.168.194.3
    podIPs:
    - ip: 192.168.194.3
    - ip: fd07:b51a:cc66:a::fd
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: a59f3a529ee91abf28ef1173d6949da391b5a2d4449a31561311d3fb3e2049c1
      checksum/secrets: 7c50797f771bff2ddd398a26e01d3cf5da19b74b83af78d45b755eed439cdccc
    creationTimestamp: "2025-01-24T06:28:31Z"
    generateName: hyperswitch-card-vault-hyperswitch-v1-55b446ccdb-
    labels:
      app: hyperswitch-card-vault
      pod-template-hash: 55b446ccdb
    name: hyperswitch-card-vault-hyperswitch-v1-55b446ccdb-5647z
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-card-vault-hyperswitch-v1-55b446ccdb
      uid: 4193f8c8-a9c0-490c-8f46-69c8eaf23671
    resourceVersion: "12787"
    uid: 635e6a89-a922-4755-9bd2-10196a5bf298
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - env:
      - name: LOCKER__LOG__CONSOLE__ENABLED
        value: "true"
      - name: LOCKER__LOG__CONSOLE__LEVEL
        value: DEBUG
      - name: LOCKER__LOG__CONSOLE__LOG_FORMAT
        value: default
      - name: LOCKER__DATABASE__USERNAME
        value: db_user
      - name: LOCKER__DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: LOCKER__DATABASE__PASSWORD
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__DATABASE__HOST
        value: hyperswitch-v1-locker-db
      - name: LOCKER__DATABASE__PORT
        value: "5432"
      - name: LOCKER__DATABASE__DBNAME
        value: locker-db
      - name: LOCKER__LIMIT__REQUEST_COUNT
        value: "100"
      - name: LOCKER__LIMIT__DURATION
        value: "60"
      - name: LOCKER__SECRETS__TENANT
        value: hyperswitch
      - name: LOCKER__SECRETS__MASTER_KEY
        valueFrom:
          secretKeyRef:
            key: LOCKER__SECRETS__MASTER_KEY
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
        valueFrom:
          secretKeyRef:
            key: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__SECRETS__TENANT_PUBLIC_KEY
        valueFrom:
          secretKeyRef:
            key: LOCKER__SECRETS__TENANT_PUBLIC_KEY
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__CACHE__MAX_CAPACITY
        value: "5000"
      - name: LOCKER__CACHE__TTI
        value: "7200"
      image: juspaydotin/hyperswitch-card-vault:v0.4.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -15 node
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: tartarus
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 50
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /local/config/development.toml
        name: hyperswitch-vault-config
        subPath: development.toml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4d8n5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-vault-role
    serviceAccountName: hyperswitch-vault-role
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: hyperswitch-vault-config-hyperswitch-v1
      name: hyperswitch-vault-config
    - name: kube-api-access-4d8n5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://8e84e6d446f3989b51adfa3d73cd5c30e5461761b962ef7bb314c5369b1ccf9f
      image: juspaydotin/hyperswitch-card-vault:v0.4.0
      imageID: docker-pullable://juspaydotin/hyperswitch-card-vault@sha256:da181997eff2fce516b49880ebb66b467c08b59b8f71e1c3b556e79d4a88b05f
      lastState: {}
      name: tartarus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.119
    podIPs:
    - ip: 192.168.194.119
    - ip: fd07:b51a:cc66:a::f3
    qosClass: Burstable
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: a59f3a529ee91abf28ef1173d6949da391b5a2d4449a31561311d3fb3e2049c1
      checksum/secrets: 7c50797f771bff2ddd398a26e01d3cf5da19b74b83af78d45b755eed439cdccc
    creationTimestamp: "2025-01-24T06:25:59Z"
    deletionGracePeriodSeconds: 120
    deletionTimestamp: "2025-01-24T06:30:31Z"
    generateName: hyperswitch-card-vault-hyperswitch-v1-55b446ccdb-
    labels:
      app: hyperswitch-card-vault
      pod-template-hash: 55b446ccdb
    name: hyperswitch-card-vault-hyperswitch-v1-55b446ccdb-d86hx
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-card-vault-hyperswitch-v1-55b446ccdb
      uid: 4193f8c8-a9c0-490c-8f46-69c8eaf23671
    resourceVersion: "12048"
    uid: 3d744605-527c-4c24-a089-6726eb57f0a8
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - env:
      - name: LOCKER__LOG__CONSOLE__ENABLED
        value: "true"
      - name: LOCKER__LOG__CONSOLE__LEVEL
        value: DEBUG
      - name: LOCKER__LOG__CONSOLE__LOG_FORMAT
        value: default
      - name: LOCKER__DATABASE__USERNAME
        value: db_user
      - name: LOCKER__DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: LOCKER__DATABASE__PASSWORD
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__DATABASE__HOST
        value: hyperswitch-v1-locker-db
      - name: LOCKER__DATABASE__PORT
        value: "5432"
      - name: LOCKER__DATABASE__DBNAME
        value: locker-db
      - name: LOCKER__LIMIT__REQUEST_COUNT
        value: "100"
      - name: LOCKER__LIMIT__DURATION
        value: "60"
      - name: LOCKER__SECRETS__TENANT
        value: hyperswitch
      - name: LOCKER__SECRETS__MASTER_KEY
        valueFrom:
          secretKeyRef:
            key: LOCKER__SECRETS__MASTER_KEY
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
        valueFrom:
          secretKeyRef:
            key: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__SECRETS__TENANT_PUBLIC_KEY
        valueFrom:
          secretKeyRef:
            key: LOCKER__SECRETS__TENANT_PUBLIC_KEY
            name: locker-secrets-hyperswitch-v1
      - name: LOCKER__CACHE__MAX_CAPACITY
        value: "5000"
      - name: LOCKER__CACHE__TTI
        value: "7200"
      image: juspaydotin/hyperswitch-card-vault:v0.4.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -15 node
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: tartarus
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 50
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /local/config/development.toml
        name: hyperswitch-vault-config
        subPath: development.toml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wmgxt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-vault-role
    serviceAccountName: hyperswitch-vault-role
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: hyperswitch-vault-config-hyperswitch-v1
      name: hyperswitch-vault-config
    - name: kube-api-access-wmgxt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:26:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:26:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:26:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:26:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:25:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://03e876803a0c774a1fb869c66df712189ecd9f49547efe18d84ea4238c4fccf2
      image: juspaydotin/hyperswitch-card-vault:v0.4.0
      imageID: docker-pullable://juspaydotin/hyperswitch-card-vault@sha256:da181997eff2fce516b49880ebb66b467c08b59b8f71e1c3b556e79d4a88b05f
      lastState: {}
      name: tartarus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:26:01Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.89
    podIPs:
    - ip: 192.168.194.89
    - ip: fd07:b51a:cc66:a::d6
    qosClass: Burstable
    startTime: "2025-01-24T06:26:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/consumer-config: 1a52e188a99d5e8019f95e8697727b13673a9a02b2c018f027256562314d7718
      checksum/consumer-secret: 7675a2e818b767925d905355e34ccd18ec440343318e2121f7c593084bcc0414
      checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
      kubectl.kubernetes.io/restartedAt: "2023-04-21T14:21:23+05:30"
    creationTimestamp: "2025-01-24T06:28:31Z"
    generateName: hyperswitch-consumer-v1o112o0-785f675ddc-
    labels:
      app: hyperswitch-consumer
      pod-template-hash: 785f675ddc
      version: consumer-v1o112o0
    name: hyperswitch-consumer-v1o112o0-785f675ddc-shtz4
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-consumer-v1o112o0-785f675ddc
      uid: c9aeea99-7670-4837-93c9-365573803a8e
    resourceVersion: "12764"
    uid: 95bc9218-c15b-49a7-bef6-80b388830057
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-type
              operator: In
              values:
              - generic-compute
    containers:
    - env:
      - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            name: hyperswitch-secrets
      - name: ROUTER__KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__ADMIN_API_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__ADMIN_API_KEY
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__JWT_SECRET
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__JWT_SECRET
            name: hyperswitch-secrets
      - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__MASTER_ENC_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__MASTER_ENC_KEY
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SENDER_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SENDER_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
        value: "1"
      - name: ROUTER__EMAIL__AWS_REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_REGION
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__HOST
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__HOST
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__PORT
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__PORT
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__USERNAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__USERNAME
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__SMTP__PASSWORD
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__PASSWORD
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            name: hyperswitch-secrets
      - name: RUN_ENV
        value: sandbox
      - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: clickhouse
      - name: ROUTER__ANALYTICS__SQLX__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      - name: ROUTER__MASTER_DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      - name: ROUTER__REPLICA_DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      envFrom:
      - secretRef:
          name: consumer-secret-hyperswitch-v1
      - secretRef:
          name: hyperswitch-secrets
      image: juspaydotin/hyperswitch-consumer:v1.112.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -15 node
      name: hyperswitch-consumer
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /local/config/sandbox.toml
        name: hyperswitch-config
        subPath: consumer.toml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85bb8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - |
        MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
          if [ $attempt -ge $MAX_ATTEMPTS ]; then
            echo "PostgreSQL did not become ready in time";
            exit 1;
          fi;
          attempt=$((attempt+1));
          echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
          sleep $SLEEP_SECONDS;
        done; echo "PostgreSQL is ready.";
      command:
      - /bin/sh
      - -c
      image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
      imagePullPolicy: IfNotPresent
      name: check-postgres
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85bb8
        readOnly: true
    - args:
      - |
        MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
          if [ $attempt -ge $MAX_ATTEMPTS ]; then
            echo "Redis did not become ready in time";
            exit 1;
          fi;
          attempt=$((attempt+1));
          echo "Waiting for Redis to be ready... Attempt: $attempt";
          sleep $SLEEP_SECONDS;
        done; echo "Redis is ready.";
      command:
      - /bin/sh
      - -c
      image: docker.io/bitnami/redis:7.2.3-debian-11-r2
      imagePullPolicy: IfNotPresent
      name: check-redis
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85bb8
        readOnly: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-router-role
    serviceAccountName: hyperswitch-router-role
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: consumer-cm-hyperswitch-v1
      name: hyperswitch-config
    - name: kube-api-access-85bb8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://99761fbcf145375877c2e887652b0272fa2e24d3bd5a038575be2dc27fbb95ea
      image: juspaydotin/hyperswitch-consumer:v1.112.1
      imageID: docker-pullable://juspaydotin/hyperswitch-consumer@sha256:8a4f374fbea2eb3af393ae6eb3cd7173c281241029a40da09ed581e9b0df4336
      lastState: {}
      name: hyperswitch-consumer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:29:00Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    initContainerStatuses:
    - containerID: docker://4a59c6782fe7fdfaa45e2283c4eef84e1fdaa601d3c9038357290cf586ca5549
      image: bitnami/postgresql:16.1.0-debian-11-r18
      imageID: docker-pullable://bitnami/postgresql@sha256:06f1f2297f6241a02bd8e8c025b31625254ca66784ac75a4a62e945fa611d045
      lastState: {}
      name: check-postgres
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://4a59c6782fe7fdfaa45e2283c4eef84e1fdaa601d3c9038357290cf586ca5549
          exitCode: 0
          finishedAt: "2025-01-24T06:28:49Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:34Z"
    - containerID: docker://335a88ff5475d87c214ea133a9403e9db00a05f9da01c88e877ded2fe249c080
      image: bitnami/redis:7.2.3-debian-11-r2
      imageID: docker-pullable://bitnami/redis@sha256:cae4e1e12b52166698ba032f919bc21c63fc9250cac060cd6fdcf8f92d86fe8b
      lastState: {}
      name: check-redis
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://335a88ff5475d87c214ea133a9403e9db00a05f9da01c88e877ded2fe249c080
          exitCode: 0
          finishedAt: "2025-01-24T06:29:00Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:50Z"
    phase: Running
    podIP: 192.168.194.118
    podIPs:
    - ip: 192.168.194.118
    - ip: fd07:b51a:cc66:a::f2
    qosClass: Burstable
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:31Z"
    generateName: hyperswitch-control-center-v1o34o2-69b889f7d8-
    labels:
      app: hyperswitch-control-center
      pod-template-hash: 69b889f7d8
      version: v1o34o2
    name: hyperswitch-control-center-v1o34o2-69b889f7d8-9szss
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-control-center-v1o34o2-69b889f7d8
      uid: 13a1d4b8-fc8a-4d4c-9f5a-57ab4f723e2b
    resourceVersion: "12430"
    uid: 516a1a4b-c227-4f9b-97b9-fb125a6a46e3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-type
              operator: In
              values:
              - generic-compute
    containers:
    - env:
      - name: binary
        value: dashboard
      - name: default__endpoints__agreement_url
        value: https://app.hyperswitch.io/agreement/tc-hyperswitch-aug-23.pdf
      - name: default__endpoints__agreement_version
        value: 1.0.0
      - name: default__endpoints__dss_certificate_url
        value: https://app.hyperswitch.io/certificates/PCI_DSS_v4-0_AOC_Juspay_2024.pdf
      - name: default__endpoints__favicon_url
      - name: default__endpoints__logo_url
      - name: default__endpoints__mixpanel_token
        value: dd4da7f62941557e716fbc0a19f9cc7e
      - name: default__features__authentication_analytics
        value: "false"
      - name: default__features__branding
        value: "false"
      - name: default__features__compliance_certificate
        value: "false"
      - name: default__features__configure_pmts
        value: "false"
      - name: default__features__custom_webhook_headers
        value: "false"
      - name: default__features__dispute_analytics
        value: "false"
      - name: default__features__dispute_evidence_upload
        value: "false"
      - name: default__features__email
        value: "true"
      - name: default__features__feedback
        value: "false"
      - name: default__features__frm
        value: "false"
      - name: default__features__generate_report
        value: "false"
      - name: default__features__global_search
        value: "false"
      - name: default__features__is_live_mode
        value: "false"
      - name: default__features__live_users_counter
        value: "false"
      - name: default__features__mixpanel
        value: "false"
      - name: default__features__payout
        value: "false"
      - name: default__features__paypal_automatic_flow
        value: "false"
      - name: default__features__performance_monitor
        value: "false"
      - name: default__features__pm_authentication_processor
        value: "false"
      - name: default__features__quick_start
        value: "false"
      - name: default__features__recon
        value: "false"
      - name: default__features__sample_data
        value: "false"
      - name: default__features__surcharge
        value: "false"
      - name: default__features__system_metrics
        value: "false"
      - name: default__features__test_live_toggle
        value: "false"
      - name: default__features__test_processors
        value: "true"
      - name: default__features__threeds_authenticator
        value: "false"
      - name: default__features__totp
        value: "true"
      - name: default__features__user_journey_analytics
        value: "false"
      - name: default__theme__primary_color
        value: '#006DF9'
      - name: default__theme__primary_hover_color
        value: '#005ED6'
      - name: default__theme__sidebar_color
        value: '#242F48'
      - name: host
        value: hyperswitch-control-center
      - name: mixpanelToken
        value: dd4da7f62941557e716fbc0a19f9cc7e
      - name: apiBaseUrl
        value: http://localhost:8080
      - name: sdkBaseUrl
        value: http://localhost:9090/web/0.103.1/v0/HyperLoader.js
      - name: default__endpoints__api_url
        value: http://localhost:8080
      - name: default__endpoints__sdk_url
        value: http://localhost:9090/web/0.103.1/v0/HyperLoader.js
      - name: default__endpoints__apple_pay_certificate_url
        value: http://localhost:8080/applepay-domain/apple-developer-merchantid-domain-association
      - name: default__features__audit_trail
        value: "true"
      image: juspaydotin/hyperswitch-control-center:v1.36.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -15 node
      name: hyperswitch-control-center
      ports:
      - containerPort: 9000
        name: http
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tkgh6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-router-role
    serviceAccountName: hyperswitch-router-role
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-tkgh6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://294409a13309d5c8476c2126bf07fc294886ed234aaa46416d6cc517f372b5c8
      image: juspaydotin/hyperswitch-control-center:v1.36.0
      imageID: docker-pullable://juspaydotin/hyperswitch-control-center@sha256:94441d142581170d27cc42f554089f9e9d0f4e3d1511d417e1eb74e311d33e5a
      lastState: {}
      name: hyperswitch-control-center
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.109
    podIPs:
    - ip: 192.168.194.109
    - ip: fd07:b51a:cc66:a::ea
    qosClass: Burstable
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
      checksum/producer-config: c26788fc47ffb9d08f324bf680a4c6bdbbff6c7c16a7283b6125251c08cb28b6
      checksum/producer-secret: 684916708382dcdd4efa45fcc8c13bd5b8195d9554834fd29daa7382659e0e52
    creationTimestamp: "2025-01-24T06:28:31Z"
    generateName: hyperswitch-producer-v1o112o0-7d88bff947-
    labels:
      app: hyperswitch-producer
      pod-template-hash: 7d88bff947
      version: v1o112o0
    name: hyperswitch-producer-v1o112o0-7d88bff947-cs55s
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-producer-v1o112o0-7d88bff947
      uid: ba133079-95c8-4bdd-8568-75ad40729497
    resourceVersion: "12766"
    uid: a04197b7-ddf2-4e14-a06a-7a34d15fefea
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-type
              operator: In
              values:
              - generic-compute
    containers:
    - env:
      - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            name: hyperswitch-secrets
      - name: ROUTER__KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__ADMIN_API_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__ADMIN_API_KEY
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__JWT_SECRET
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__JWT_SECRET
            name: hyperswitch-secrets
      - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__MASTER_ENC_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__MASTER_ENC_KEY
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SENDER_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SENDER_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
        value: "1"
      - name: ROUTER__EMAIL__AWS_REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_REGION
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__HOST
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__HOST
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__PORT
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__PORT
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__USERNAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__USERNAME
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__SMTP__PASSWORD
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__PASSWORD
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            name: hyperswitch-secrets
      - name: RUN_ENV
        value: sandbox
      - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: clickhouse
      - name: ROUTER__ANALYTICS__SQLX__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      - name: ROUTER__MASTER_DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      - name: ROUTER__REPLICA_DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      envFrom:
      - secretRef:
          name: producer-secret-hyperswitch-v1
      - secretRef:
          name: hyperswitch-secrets
      image: juspaydotin/hyperswitch-producer:v1.112.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -15 node
      name: hyperswitch-producer
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /local/config/sandbox.toml
        name: hyperswitch-config
        subPath: producer.toml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hvcrk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - |
        MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
          if [ $attempt -ge $MAX_ATTEMPTS ]; then
            echo "PostgreSQL did not become ready in time";
            exit 1;
          fi;
          attempt=$((attempt+1));
          echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
          sleep $SLEEP_SECONDS;
        done; echo "PostgreSQL is ready.";
      command:
      - /bin/sh
      - -c
      image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
      imagePullPolicy: IfNotPresent
      name: check-postgres
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hvcrk
        readOnly: true
    - args:
      - |
        MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
          if [ $attempt -ge $MAX_ATTEMPTS ]; then
            echo "Redis did not become ready in time";
            exit 1;
          fi;
          attempt=$((attempt+1));
          echo "Waiting for Redis to be ready... Attempt: $attempt";
          sleep $SLEEP_SECONDS;
        done; echo "Redis is ready.";
      command:
      - /bin/sh
      - -c
      image: docker.io/bitnami/redis:7.2.3-debian-11-r2
      imagePullPolicy: IfNotPresent
      name: check-redis
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hvcrk
        readOnly: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-router-role
    serviceAccountName: hyperswitch-router-role
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: producer-cm-hyperswitch-v1
      name: hyperswitch-config
    - name: kube-api-access-hvcrk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://efb874635392b4bc2303c81ed4a26cd664cbb387e56ff6381025465d9deebe8b
      image: juspaydotin/hyperswitch-producer:v1.112.1
      imageID: docker-pullable://juspaydotin/hyperswitch-producer@sha256:43167d6c0bb33cc7ded5ab29db2a5cf6416f97d7a58e35e44fbcb2bfefe89041
      lastState: {}
      name: hyperswitch-producer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:29:00Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    initContainerStatuses:
    - containerID: docker://d1af5cd71844b855580bd1c4ce9f893fb81ac5210a5b96c12e95dc2871a3f315
      image: bitnami/postgresql:16.1.0-debian-11-r18
      imageID: docker-pullable://bitnami/postgresql@sha256:06f1f2297f6241a02bd8e8c025b31625254ca66784ac75a4a62e945fa611d045
      lastState: {}
      name: check-postgres
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://d1af5cd71844b855580bd1c4ce9f893fb81ac5210a5b96c12e95dc2871a3f315
          exitCode: 0
          finishedAt: "2025-01-24T06:28:49Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:34Z"
    - containerID: docker://b185617aaf0269a525b341dc13d5d8c8b0fe0bcd49a79563804fd8e0ced1356c
      image: bitnami/redis:7.2.3-debian-11-r2
      imageID: docker-pullable://bitnami/redis@sha256:cae4e1e12b52166698ba032f919bc21c63fc9250cac060cd6fdcf8f92d86fe8b
      lastState: {}
      name: check-redis
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://b185617aaf0269a525b341dc13d5d8c8b0fe0bcd49a79563804fd8e0ced1356c
          exitCode: 0
          finishedAt: "2025-01-24T06:29:00Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:50Z"
    phase: Running
    podIP: 192.168.194.117
    podIPs:
    - ip: 192.168.194.117
    - ip: fd07:b51a:cc66:a::f5
    qosClass: Burstable
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
      checksum/router-config: c17d39eed43a8eb1b1b6406f54b5596ffa6eb07993ad56c075c8c5e7e51ff2fc
      checksum/router-secret: 09bf14ce697665acaf53ee21bda701cd770270810e0b0ba50f219491a3f1ac5c
      kubectl.kubernetes.io/restartedAt: "2023-09-20T12:11:41+05:30"
      traffic.sidecar.istio.io/excludeOutboundIPRanges: 10.23.6.12/32
    creationTimestamp: "2025-01-24T06:28:31Z"
    generateName: hyperswitch-server-v1o112o0-867fc9666f-
    labels:
      app: hyperswitch-server
      pod-template-hash: 867fc9666f
      version: v1o112o0
    name: hyperswitch-server-v1o112o0-867fc9666f-dt9hf
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-server-v1o112o0-867fc9666f
      uid: c536ca94-a42d-4b5a-af30-c835ae013087
    resourceVersion: "12798"
    uid: 344a7626-d929-42dc-a394-955ba98f7345
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-type
              operator: In
              values:
              - generic-compute
    containers:
    - env:
      - name: BINARY
        value: router
      - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            name: hyperswitch-secrets
      - name: ROUTER__KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__ADMIN_API_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__ADMIN_API_KEY
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__JWT_SECRET
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__JWT_SECRET
            name: hyperswitch-secrets
      - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS__MASTER_ENC_KEY
        valueFrom:
          secretKeyRef:
            key: ROUTER__SECRETS__MASTER_ENC_KEY
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            name: hyperswitch-secrets
      - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__KEY_ID
            name: hyperswitch-secrets
      - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__KMS__REGION
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            name: hyperswitch-secrets
      - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
        valueFrom:
          secretKeyRef:
            key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SENDER_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SENDER_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
        value: "1"
      - name: ROUTER__EMAIL__AWS_REGION
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_REGION
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__HOST
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__HOST
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__PORT
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__PORT
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__SMTP__USERNAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__USERNAME
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__SMTP__PASSWORD
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__SMTP__PASSWORD
            name: hyperswitch-secrets
            optional: true
      - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            name: hyperswitch-secrets
      - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
        valueFrom:
          secretKeyRef:
            key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            name: hyperswitch-secrets
      - name: RUN_ENV
        value: sandbox
      - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: clickhouse
      - name: ROUTER__ANALYTICS__SQLX__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      - name: ROUTER__MASTER_DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      - name: ROUTER__REPLICA_DATABASE__PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      envFrom:
      - configMapRef:
          name: router-cm-hyperswitch-v1
      - secretRef:
          name: router-secret-hyperswitch-v1
      - secretRef:
          name: hyperswitch-secrets
      image: juspaydotin/hyperswitch-router:v1.112.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -15 node
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: hyperswitch-router
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 200m
          memory: 500Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /local/config/sandbox.toml
        name: hyperswitch-config
        subPath: router.toml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pdnth
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - |
        MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
          if [ $attempt -ge $MAX_ATTEMPTS ]; then
            echo "PostgreSQL did not become ready in time";
            exit 1;
          fi;
          attempt=$((attempt+1));
          echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
          sleep $SLEEP_SECONDS;
        done; echo "PostgreSQL is ready.";
      command:
      - /bin/sh
      - -c
      image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
      imagePullPolicy: IfNotPresent
      name: check-postgres
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pdnth
        readOnly: true
    - args:
      - |
        MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
          if [ $attempt -ge $MAX_ATTEMPTS ]; then
            echo "Redis did not become ready in time";
            exit 1;
          fi;
          attempt=$((attempt+1));
          echo "Waiting for Redis to be ready... Attempt: $attempt";
          sleep $SLEEP_SECONDS;
        done; echo "Redis is ready.";
      command:
      - /bin/sh
      - -c
      image: docker.io/bitnami/redis:7.2.3-debian-11-r2
      imagePullPolicy: IfNotPresent
      name: check-redis
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pdnth
        readOnly: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-router-role
    serviceAccountName: hyperswitch-router-role
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: router-cm-hyperswitch-v1
      name: hyperswitch-config
    - name: kube-api-access-pdnth
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://bee12a1b15329da52c0e52cb9c2ef30e7132db96a61560d2bcda5982472377c4
      image: juspaydotin/hyperswitch-router:v1.112.1
      imageID: docker-pullable://juspaydotin/hyperswitch-router@sha256:4b8c89ca18c344ecd2dad2cf3b833d2f6d7bd6402e3911fb4e9f1fa57252d3fc
      lastState: {}
      name: hyperswitch-router
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:29:00Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    initContainerStatuses:
    - containerID: docker://5a05d72bb8b6e38a56493ef7adb89e7de425d7f66eb9ac21198bcd525f8b8e3b
      image: bitnami/postgresql:16.1.0-debian-11-r18
      imageID: docker-pullable://bitnami/postgresql@sha256:06f1f2297f6241a02bd8e8c025b31625254ca66784ac75a4a62e945fa611d045
      lastState: {}
      name: check-postgres
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://5a05d72bb8b6e38a56493ef7adb89e7de425d7f66eb9ac21198bcd525f8b8e3b
          exitCode: 0
          finishedAt: "2025-01-24T06:28:49Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:34Z"
    - containerID: docker://2598f82bf02a5387490487f6f6d67441db70cb9b63a2bbb1dc6f328102018b53
      image: bitnami/redis:7.2.3-debian-11-r2
      imageID: docker-pullable://bitnami/redis@sha256:cae4e1e12b52166698ba032f919bc21c63fc9250cac060cd6fdcf8f92d86fe8b
      lastState: {}
      name: check-redis
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://2598f82bf02a5387490487f6f6d67441db70cb9b63a2bbb1dc6f328102018b53
          exitCode: 0
          finishedAt: "2025-01-24T06:29:00Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:50Z"
    phase: Running
    podIP: 192.168.194.116
    podIPs:
    - ip: 192.168.194.116
    - ip: fd07:b51a:cc66:a::f1
    qosClass: Burstable
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 322df4b327f90632afc999b8103eaa8db5c5b439456313837a9f8e936616cfec
      checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      checksum/secret: 41fa47582aada96cfd80f98d18f43725edec86c8243f67111c5621507e907350
    creationTimestamp: "2025-01-24T06:28:31Z"
    generateName: hyperswitch-v1-grafana-5c95d79b6-
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: grafana
      pod-template-hash: 5c95d79b6
    name: hyperswitch-v1-grafana-5c95d79b6-p4p2f
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-v1-grafana-5c95d79b6
      uid: 477eb684-2703-4c24-841f-00f88e117d78
    resourceVersion: "12670"
    uid: dd06cda2-6f58-4d5f-8b55-d90ea10c70be
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: hyperswitch-v1-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: hyperswitch-v1-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.19.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7bmzl
        readOnly: true
    - env:
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: hyperswitch-v1-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: hyperswitch-v1-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: grafana/grafana:10.0.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7bmzl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsUser: 472
    serviceAccount: hyperswitch-v1-grafana
    serviceAccountName: hyperswitch-v1-grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: hyperswitch-v1-grafana
      name: config
    - emptyDir: {}
      name: storage
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-7bmzl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a2f5ecb41e0d9c73a6c937f447e7fdebf7d0f62ea7adf5a2b4a529cf4ddf0754
      image: grafana/grafana:10.0.1
      imageID: docker-pullable://grafana/grafana@sha256:c2a9d25b77b9a7439e56efffa916e43eda09db4f7b78526082443f9c2ee18dc0
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    - containerID: docker://d6e7fa9937147b6bee2f4b0c9778d814ae353f8635382476ec9497c293be5fce
      image: quay.io/kiwigrid/k8s-sidecar:1.19.2
      imageID: docker-pullable://quay.io/kiwigrid/k8s-sidecar@sha256:6a8671702d6f8651c11bee1cd9a24d3dde6a5a05e0972d91c35009c38b527616
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.115
    podIPs:
    - ip: 192.168.194.115
    - ip: fd07:b51a:cc66:a::f0
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:33Z"
    generateName: hyperswitch-v1-locker-db-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: locker-db
      app.kubernetes.io/version: 16.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: hyperswitch-v1-locker-db-8444b58bbf
      helm.sh/chart: postgresql-13.2.27
      statefulset.kubernetes.io/pod-name: hyperswitch-v1-locker-db-0
    name: hyperswitch-v1-locker-db-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: hyperswitch-v1-locker-db
      uid: f0795c6e-e08d-4cb0-b31a-f8259d84f897
    resourceVersion: "12682"
    uid: 70b8a325-91f7-4cbd-869e-d24630ba02bd
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: hyperswitch-v1
                app.kubernetes.io/name: locker-db
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: db_user
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-locker-db
      - name: POSTGRES_POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: hyperswitch-v1-locker-db
      - name: POSTGRES_DATABASE
        value: locker-db
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "db_user" -d "dbname=locker-db" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "db_user" -d "dbname=locker-db" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-89kcd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: hyperswitch-v1-locker-db-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: default
    serviceAccountName: default
    subdomain: hyperswitch-v1-locker-db-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-hyperswitch-v1-locker-db-0
    - emptyDir:
        medium: Memory
      name: dshm
    - name: kube-api-access-89kcd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a51d8f526b92fd16592c03fc1c0538ca8079d7291a54953b9547cad3f91a9a63
      image: bitnami/postgresql:16.1.0-debian-11-r18
      imageID: docker-pullable://bitnami/postgresql@sha256:06f1f2297f6241a02bd8e8c025b31625254ca66784ac75a4a62e945fa611d045
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.121
    podIPs:
    - ip: 192.168.194.121
    - ip: fd07:b51a:cc66:a::f4
    qosClass: Burstable
    startTime: "2025-01-24T06:28:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:38Z"
    generateName: hyperswitch-v1-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: hyperswitch-v1-postgresql-64ccf8b5f
      helm.sh/chart: postgresql-13.2.27
      statefulset.kubernetes.io/pod-name: hyperswitch-v1-postgresql-0
    name: hyperswitch-v1-postgresql-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: hyperswitch-v1-postgresql
      uid: 2f18f424-980d-46de-8499-f03b0a7e1d0a
    resourceVersion: "12710"
    uid: 20010d29-e567-40fb-aab2-436fb5db992f
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: hyperswitch-v1
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: hyperswitch
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: hyperswitch-v1-postgresql
      - name: POSTGRES_POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: hyperswitch-v1-postgresql
      - name: POSTGRES_DATABASE
        value: hyperswitch
      - name: POSTGRES_REPLICATION_MODE
        value: master
      - name: POSTGRES_REPLICATION_USER
        value: repl_user
      - name: POSTGRES_REPLICATION_PASSWORD
        valueFrom:
          secretKeyRef:
            key: replication-password
            name: hyperswitch-v1-postgresql
      - name: POSTGRES_CLUSTER_APP_NAME
        value: my_application
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "hyperswitch" -d "dbname=hyperswitch" -h 127.0.0.1
            -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "hyperswitch" -d "dbname=hyperswitch" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 150m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fcqq4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: hyperswitch-v1-postgresql-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: default
    serviceAccountName: default
    subdomain: hyperswitch-v1-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-hyperswitch-v1-postgresql-0
    - emptyDir:
        medium: Memory
      name: dshm
    - name: kube-api-access-fcqq4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://26a0d5c9347838011f258e191cccd0a27d82313bba840f5bd823a9fa94d78d82
      image: bitnami/postgresql:16.1.0-debian-11-r18
      imageID: docker-pullable://bitnami/postgresql@sha256:06f1f2297f6241a02bd8e8c025b31625254ca66784ac75a4a62e945fa611d045
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:38Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.5
    podIPs:
    - ip: 192.168.194.5
    - ip: fd07:b51a:cc66:a::ff
    qosClass: Burstable
    startTime: "2025-01-24T06:28:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 30f5c7b874124b12826873762af93f265f37fff1f305930c16692b43d107e707
    creationTimestamp: "2025-01-24T06:28:33Z"
    generateName: hyperswitch-v1-promtail-
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: promtail
      controller-revision-hash: 865df7856b
      pod-template-generation: "1"
    name: hyperswitch-v1-promtail-5nbmq
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: hyperswitch-v1-promtail
      uid: 81994c17-2e6a-4b66-8da6-e7aaf9e96613
    resourceVersion: "12821"
    uid: a94f1657-b8d5-43d2-8608-1ac3ceb486d1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nrvqx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: hyperswitch-v1-promtail
    serviceAccountName: hyperswitch-v1-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: hyperswitch-v1-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-nrvqx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:54Z"
      message: 'containers with unready status: [promtail]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:54Z"
      message: 'containers with unready status: [promtail]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://608e0e01a9cf5c65828147dd060b766804a4ad5c69a2598b955cfeb0106d39cb
      image: grafana/promtail:2.9.3
      imageID: docker-pullable://grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState: {}
      name: promtail
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.122
    podIPs:
    - ip: 192.168.194.122
    - ip: fd07:b51a:cc66:a::f7
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
      checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
      checksum/scripts: 1a2a98d8e7da56e19dce6d5c5eff0a4b986298ec80143cde0afb2aa6a02d9db8
      checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    creationTimestamp: "2025-01-24T06:28:33Z"
    generateName: hyperswitch-v1-redis-master-
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.3
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: hyperswitch-v1-redis-master-5678dcb954
      helm.sh/chart: redis-18.6.1
      statefulset.kubernetes.io/pod-name: hyperswitch-v1-redis-master-0
    name: hyperswitch-v1-redis-master-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: hyperswitch-v1-redis-master
      uid: 818258e4-f59a-40e8-8b25-52ba6d259a1e
    resourceVersion: "12746"
    uid: bd3fbe4e-87db-4ed9-b1c1-b28ad564499a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: master
                app.kubernetes.io/instance: hyperswitch-v1
                app.kubernetes.io/name: redis
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: true
    containers:
    - args:
      - -c
      - /opt/bitnami/scripts/start-scripts/start-master.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: REDIS_REPLICATION_MODE
        value: master
      - name: ALLOW_EMPTY_PASSWORD
        value: "yes"
      - name: REDIS_TLS_ENABLED
        value: "no"
      - name: REDIS_PORT
        value: "6379"
      image: docker.io/bitnami/redis:7.2.3-debian-11-r2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_liveness_local.sh 5
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 6
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /health/ping_readiness_local.sh 1
        failureThreshold: 5
        initialDelaySeconds: 20
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        runAsGroup: 0
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/scripts/start-scripts
        name: start-scripts
      - mountPath: /health
        name: health
      - mountPath: /data
        name: redis-data
      - mountPath: /opt/bitnami/redis/mounted-etc
        name: config
      - mountPath: /opt/bitnami/redis/etc/
        name: redis-tmp-conf
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pnzmd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: hyperswitch-v1-redis-master-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: hyperswitch-v1-redis
    serviceAccountName: hyperswitch-v1-redis
    subdomain: hyperswitch-v1-redis-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: redis-data
      persistentVolumeClaim:
        claimName: redis-data-hyperswitch-v1-redis-master-0
    - configMap:
        defaultMode: 493
        name: hyperswitch-v1-redis-scripts
      name: start-scripts
    - configMap:
        defaultMode: 493
        name: hyperswitch-v1-redis-health
      name: health
    - configMap:
        defaultMode: 420
        name: hyperswitch-v1-redis-configuration
      name: config
    - emptyDir: {}
      name: redis-tmp-conf
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-pnzmd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6827488a2ef3dce92605f8d4201e51d6caeed20ea56d97b29da8e195593226c4
      image: bitnami/redis:7.2.3-debian-11-r2
      imageID: docker-pullable://bitnami/redis@sha256:cae4e1e12b52166698ba032f919bc21c63fc9250cac060cd6fdcf8f92d86fe8b
      lastState: {}
      name: redis
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.111
    podIPs:
    - ip: 192.168.194.111
    - ip: fd07:b51a:cc66:a::ec
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2b39e38c4ca14a82b11b27b4fa71b9062af3e91bada795fe8fc4fcb190cd21c1
    creationTimestamp: "2025-01-24T06:28:36Z"
    generateName: hyperswitch-v1-vector-
    labels:
      app.kubernetes.io/component: Aggregator
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: vector
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: hyperswitch-v1-vector-f87d7b4bd
      statefulset.kubernetes.io/pod-name: hyperswitch-v1-vector-0
      vector.dev/exclude: "true"
    name: hyperswitch-v1-vector-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: hyperswitch-v1-vector
      uid: e48b8604-6351-4076-9653-18b85b857e5c
    resourceVersion: "12647"
    uid: e818af20-613e-4673-8dd9-de41cfd36b91
  spec:
    containers:
    - args:
      - --config-dir
      - /etc/vector/
      env:
      - name: VECTOR_LOG
        value: info
      - name: KAFKA_HOST
        value: kafka0:29092
      image: timberio/vector:0.42.0-distroless-libc
      imagePullPolicy: IfNotPresent
      name: vector
      ports:
      - containerPort: 8686
        name: api
        protocol: TCP
      - containerPort: 3103
        name: sdk-source
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /vector-data-dir
        name: data
      - mountPath: /etc/vector/
        name: config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fb9sg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: hyperswitch-v1-vector-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-v1-vector
    serviceAccountName: hyperswitch-v1-vector
    subdomain: hyperswitch-v1-vector-headless
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - name: config
      projected:
        defaultMode: 420
        sources:
        - configMap:
            name: hyperswitch-v1-vector
    - name: kube-api-access-fb9sg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://33d5ef2e09f460f71d5c754c2c612f67a2df6edc3887f9f3d8c147b8a665087c
      image: timberio/vector:0.42.0-distroless-libc
      imageID: docker-pullable://timberio/vector@sha256:252e66723a0921c1e6bdfc057bb7d1181078f5c17cdfc19a7a9db98283da30e5
      lastState: {}
      name: vector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:37Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.4
    podIPs:
    - ip: 192.168.194.4
    - ip: fd07:b51a:cc66:a::fe
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:34Z"
    generateName: hyperswitch-v1-zookeeper-
    labels:
      app.kubernetes.io/component: zookeeper
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/version: 3.9.3
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: hyperswitch-v1-zookeeper-65dbb579dc
      helm.sh/chart: zookeeper-13.6.0
      statefulset.kubernetes.io/pod-name: hyperswitch-v1-zookeeper-0
    name: hyperswitch-v1-zookeeper-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: hyperswitch-v1-zookeeper
      uid: 839bd331-8c9f-4d70-8c52-876c7316b483
    resourceVersion: "12703"
    uid: 70858f2a-f277-4e7c-917f-b35f86c7a600
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: zookeeper
                app.kubernetes.io/instance: hyperswitch-v1
                app.kubernetes.io/name: zookeeper
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - command:
      - /scripts/setup.sh
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: ZOO_DATA_LOG_DIR
      - name: ZOO_PORT_NUMBER
        value: "2181"
      - name: ZOO_TICK_TIME
        value: "2000"
      - name: ZOO_INIT_LIMIT
        value: "10"
      - name: ZOO_SYNC_LIMIT
        value: "5"
      - name: ZOO_PRE_ALLOC_SIZE
        value: "65536"
      - name: ZOO_SNAPCOUNT
        value: "100000"
      - name: ZOO_MAX_CLIENT_CNXNS
        value: "60"
      - name: ZOO_4LW_COMMANDS_WHITELIST
        value: srvr, mntr, ruok
      - name: ZOO_LISTEN_ALLIPS_ENABLED
        value: "no"
      - name: ZOO_AUTOPURGE_INTERVAL
        value: "1"
      - name: ZOO_AUTOPURGE_RETAIN_COUNT
        value: "10"
      - name: ZOO_MAX_SESSION_TIMEOUT
        value: "40000"
      - name: ZOO_SERVERS
        value: hyperswitch-v1-zookeeper-0.hyperswitch-v1-zookeeper-headless.hyperswitch.svc.cluster.local:2888:3888::1
      - name: ZOO_ENABLE_AUTH
        value: "no"
      - name: ZOO_ENABLE_QUORUM_AUTH
        value: "no"
      - name: ZOO_HEAP_SIZE
        value: "1024"
      - name: ZOO_LOG_LEVEL
        value: ERROR
      - name: ALLOW_ANONYMOUS_LOGIN
        value: "yes"
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: ZOO_ADMIN_SERVER_PORT_NUMBER
        value: "8080"
      image: docker.io/bitnami/zookeeper:3.8.4-debian-12-r16
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/bash
          - -ec
          - ZOO_HC_TIMEOUT=3 /opt/bitnami/scripts/zookeeper/healthcheck.sh
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: zookeeper
      ports:
      - containerPort: 2181
        name: client
        protocol: TCP
      - containerPort: 8080
        name: http-admin
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/bash
          - -ec
          - ZOO_HC_TIMEOUT=2 /opt/bitnami/scripts/zookeeper/healthcheck.sh
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 375m
          ephemeral-storage: 2Gi
          memory: 384Mi
        requests:
          cpu: 250m
          ephemeral-storage: 50Mi
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/zookeeper/conf
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /opt/bitnami/zookeeper/logs
        name: empty-dir
        subPath: app-logs-dir
      - mountPath: /scripts/setup.sh
        name: scripts
        subPath: setup.sh
      - mountPath: /bitnami/zookeeper
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: hyperswitch-v1-zookeeper-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: hyperswitch-v1-zookeeper
    serviceAccountName: hyperswitch-v1-zookeeper
    subdomain: hyperswitch-v1-zookeeper-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-hyperswitch-v1-zookeeper-0
    - emptyDir: {}
      name: empty-dir
    - configMap:
        defaultMode: 493
        name: hyperswitch-v1-zookeeper-scripts
      name: scripts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:36Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://777a1dc41b251e5ff3fa8a2013a18d9849544d10327bb2797b909c67dee637ba
      image: bitnami/zookeeper:3.8.4-debian-12-r16
      imageID: docker-pullable://bitnami/zookeeper@sha256:533595684c631c720a02a2804037d032b7b1b08c6f5aea1c6932b76115200dfc
      lastState: {}
      name: zookeeper
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:35Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.126
    podIPs:
    - ip: 192.168.194.126
    - ip: fd07:b51a:cc66:a::fb
    qosClass: Burstable
    startTime: "2025-01-24T06:28:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helm.sh/hook: post-install,post-upgrade
      helm.sh/hook-delete-policy: hook-succeeded
      helm.sh/hook-weight: "-5"
    creationTimestamp: "2025-01-24T06:28:32Z"
    generateName: hyperswitch-web-5475d55478-
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hyperswitch-web
      app.kubernetes.io/version: 0.15.8
      helm.sh/chart: hyperswitch-web-0.2.0
      pod-template-hash: 5475d55478
    name: hyperswitch-web-5475d55478-m5fqv
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hyperswitch-web-5475d55478
      uid: 647ff65c-b047-4888-bddd-ed6183d22ebb
    resourceVersion: "12463"
    uid: d79c2af7-d529-426e-bfdd-ebade067bad1
  spec:
    containers:
    - envFrom:
      - configMapRef:
          name: hyperswitch-web-nginx
      image: juspaydotin/hyperswitch-web:0.103.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /web/0.103.1/v0/
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: hyperswitch-web
      ports:
      - containerPort: 9090
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /web/0.103.1/v0/
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 3Gi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/nginx/conf.d/default.conf
        name: nginx-config-volume
        subPath: default.conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l9pj5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hyperswitch-web
    serviceAccountName: hyperswitch-web
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: hyperswitch-web-nginx
      name: nginx-config-volume
    - name: kube-api-access-l9pj5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://0302b52b13cf79ecac93b072cc1083d7f5b8cd0bf420f86e674b3ff1c92b4eb3
      image: juspaydotin/hyperswitch-web:0.103.1
      imageID: docker-pullable://juspaydotin/hyperswitch-web@sha256:527a05d1f5cc8e18e33ca22f61ca7220dec70e76d5ae2d415bd2d155add858db
      lastState: {}
      name: hyperswitch-web
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:33Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.110
    podIPs:
    - ip: 192.168.194.110
    - ip: fd07:b51a:cc66:a::eb
    qosClass: Burstable
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configuration: e2338a6a5df0ed29e2a86beac8a45e32a6d5bd3c0f06f968c0112522df877db7
    creationTimestamp: "2025-01-24T06:28:34Z"
    generateName: kafka0-broker-
    labels:
      app.kubernetes.io/component: broker
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.9.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: kafka0-broker-58584c8697
      helm.sh/chart: kafka-31.0.0
      statefulset.kubernetes.io/pod-name: kafka0-broker-0
    name: kafka0-broker-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka0-broker
      uid: e715a825-f9c7-44bf-8c04-941678350f01
    resourceVersion: "12690"
    uid: e3ec5f18-676d-459f-8941-22f394d73244
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: broker
                app.kubernetes.io/instance: hyperswitch-v1
                app.kubernetes.io/name: kafka
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: KAFKA_HEAP_OPTS
        value: -Xmx1024m -Xms1024m
      - name: KAFKA_KRAFT_CLUSTER_ID
        valueFrom:
          secretKeyRef:
            key: kraft-cluster-id
            name: kafka0-kraft-cluster-id
      image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - pgrep
          - -f
          - kafka
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kafka
      ports:
      - containerPort: 9092
        name: client
        protocol: TCP
      - containerPort: 9094
        name: interbroker
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: client
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /opt/bitnami/kafka/config/server.properties
        name: kafka-config
        subPath: server.properties
      - mountPath: /tmp
        name: tmp
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka0-broker-0
    initContainers:
    - args:
      - -ec
      - |
        /scripts/kafka-init.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: KAFKA_VOLUME_DIR
        value: /bitnami/kafka
      - name: KAFKA_MIN_ID
        value: "100"
      image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
      imagePullPolicy: IfNotPresent
      name: kafka-init
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /config
        name: kafka-config
      - mountPath: /configmaps
        name: kafka-configmaps
      - mountPath: /secret-config
        name: kafka-secret-config
      - mountPath: /scripts
        name: scripts
      - mountPath: /tmp
        name: tmp
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kafka0
    serviceAccountName: kafka0
    subdomain: kafka0-broker-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-kafka0-broker-0
    - configMap:
        defaultMode: 420
        name: kafka0-broker-configuration
      name: kafka-configmaps
    - emptyDir: {}
      name: kafka-secret-config
    - emptyDir: {}
      name: kafka-config
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 493
        name: kafka0-scripts
      name: scripts
    - emptyDir: {}
      name: logs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://90720527f84db19320ce34c26606dbf876755b1399d85bc343984440fcfcff8f
      image: bitnami/kafka:3.9.0-debian-12-r1
      imageID: docker-pullable://bitnami/kafka@sha256:9b38734577ac16445cc49db93f41a8f7effca24aa9cbcdd798edbdbb418e0601
      lastState: {}
      name: kafka
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:36Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    initContainerStatuses:
    - containerID: docker://8e994e03de2fbf66b44ba7c52a7a1a1c5f04516e88f038f2230aeed5197daba5
      image: bitnami/kafka:3.9.0-debian-12-r1
      imageID: docker-pullable://bitnami/kafka@sha256:9b38734577ac16445cc49db93f41a8f7effca24aa9cbcdd798edbdbb418e0601
      lastState: {}
      name: kafka-init
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://8e994e03de2fbf66b44ba7c52a7a1a1c5f04516e88f038f2230aeed5197daba5
          exitCode: 0
          finishedAt: "2025-01-24T06:28:35Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:35Z"
    phase: Running
    podIP: 192.168.194.123
    podIPs:
    - ip: 192.168.194.123
    - ip: fd07:b51a:cc66:a::f8
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configuration: 42fd96e136d37d8e7074e5c26e823e42038d92b27c338ecd3de1b915d0da2f94
    creationTimestamp: "2025-01-24T06:28:34Z"
    generateName: kafka0-controller-
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.9.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: kafka0-controller-745f5b8ccf
      helm.sh/chart: kafka-31.0.0
      statefulset.kubernetes.io/pod-name: kafka0-controller-0
    name: kafka0-controller-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka0-controller
      uid: 66686a07-fdb5-4429-8203-e3b27b1a4883
    resourceVersion: "12696"
    uid: 75a59d88-304e-49de-9b17-d511dcf53aec
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: controller-eligible
                app.kubernetes.io/instance: hyperswitch-v1
                app.kubernetes.io/name: kafka
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: KAFKA_HEAP_OPTS
        value: -Xmx1024m -Xms1024m
      - name: KAFKA_KRAFT_CLUSTER_ID
        valueFrom:
          secretKeyRef:
            key: kraft-cluster-id
            name: kafka0-kraft-cluster-id
      image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - pgrep
          - -f
          - kafka
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kafka
      ports:
      - containerPort: 9093
        name: controller
        protocol: TCP
      - containerPort: 9092
        name: client
        protocol: TCP
      - containerPort: 9094
        name: interbroker
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: controller
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /opt/bitnami/kafka/logs
        name: logs
      - mountPath: /opt/bitnami/kafka/config/server.properties
        name: kafka-config
        subPath: server.properties
      - mountPath: /tmp
        name: tmp
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka0-controller-0
    initContainers:
    - args:
      - -ec
      - |
        /scripts/kafka-init.sh
      command:
      - /bin/bash
      env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: KAFKA_VOLUME_DIR
        value: /bitnami/kafka
      - name: KAFKA_MIN_ID
        value: "0"
      image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
      imagePullPolicy: IfNotPresent
      name: kafka-init
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bitnami/kafka
        name: data
      - mountPath: /config
        name: kafka-config
      - mountPath: /configmaps
        name: kafka-configmaps
      - mountPath: /secret-config
        name: kafka-secret-config
      - mountPath: /scripts
        name: scripts
      - mountPath: /tmp
        name: tmp
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kafka0
    serviceAccountName: kafka0
    subdomain: kafka0-controller-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-kafka0-controller-0
    - configMap:
        defaultMode: 420
        name: kafka0-controller-configuration
      name: kafka-configmaps
    - emptyDir: {}
      name: kafka-secret-config
    - emptyDir: {}
      name: kafka-config
    - emptyDir: {}
      name: tmp
    - configMap:
        defaultMode: 493
        name: kafka0-scripts
      name: scripts
    - emptyDir: {}
      name: logs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://830a1755bb9d49a44e67447f05b7193c651f0669327c6a09d8b7e7d9ad2acc47
      image: bitnami/kafka:3.9.0-debian-12-r1
      imageID: docker-pullable://bitnami/kafka@sha256:9b38734577ac16445cc49db93f41a8f7effca24aa9cbcdd798edbdbb418e0601
      lastState: {}
      name: kafka
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:36Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    initContainerStatuses:
    - containerID: docker://e222b5eea6cf1e5dddfd1ef16b31b39685cbacf3586d473d8c8d6dc2a0b79125
      image: bitnami/kafka:3.9.0-debian-12-r1
      imageID: docker-pullable://bitnami/kafka@sha256:9b38734577ac16445cc49db93f41a8f7effca24aa9cbcdd798edbdbb418e0601
      lastState: {}
      name: kafka-init
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: docker://e222b5eea6cf1e5dddfd1ef16b31b39685cbacf3586d473d8c8d6dc2a0b79125
          exitCode: 0
          finishedAt: "2025-01-24T06:28:35Z"
          reason: Completed
          startedAt: "2025-01-24T06:28:35Z"
    phase: Running
    podIP: 192.168.194.2
    podIPs:
    - ip: 192.168.194.2
    - ip: fd07:b51a:cc66:a::fc
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 710cdc54bd9200910bfcca95c22c8a53056e7b37cd3ac7be301da54dbee52dc9
      prometheus.io/port: http-metrics
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-01-24T06:28:34Z"
    generateName: loki-
    labels:
      app: loki
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-564bc8cc5b
      name: loki
      release: hyperswitch-v1
      statefulset.kubernetes.io/pod-name: loki-0
    name: loki-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki
      uid: 43a7d275-7730-4efb-9e22-4f809c52e953
    resourceVersion: "12810"
    uid: e3a665b2-dff5-40c7-ba06-10b89cc21c0f
  spec:
    affinity: {}
    containers:
    - args:
      - -config.file=/etc/loki/loki.yaml
      image: grafana/loki:2.6.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: memberlist-port
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8rpd2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-headless
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: config
      secret:
        defaultMode: 420
        secretName: loki
    - emptyDir: {}
      name: storage
    - name: kube-api-access-8rpd2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:36Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ea96f7ee456d2c4256ff3b9a57fd62ffe0e1e8c6cd9cb218635937e41f016236
      image: grafana/loki:2.6.1
      imageID: docker-pullable://grafana/loki@sha256:1ee60f980950b00e505bd564b40f720132a0653b110e993043bb5940673d060a
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:35Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.125
    podIPs:
    - ip: 192.168.194.125
    - ip: fd07:b51a:cc66:a::fa
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:32Z"
    generateName: mailhog-765485df76-
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: mailhog
      pod-template-hash: 765485df76
    name: mailhog-765485df76-286z6
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: mailhog-765485df76
      uid: 8517d8d2-6a65-432d-b9ee-0b270a196e74
    resourceVersion: "12471"
    uid: 3b9e42b8-f692-4c5d-97fb-72006ad76afd
  spec:
    automountServiceAccountToken: false
    containers:
    - env:
      - name: MH_HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: docker.io/mailhog/mailhog:v1.0.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: tcp-smtp
        timeoutSeconds: 1
      name: mailhog
      ports:
      - containerPort: 8025
        name: http
        protocol: TCP
      - containerPort: 1025
        name: tcp-smtp
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: tcp-smtp
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: mailhog
    serviceAccountName: mailhog
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://bcce2f8a4412ca3d7122859347472ec1c09778cccfdbd9a9cd9a8858d5cb1c98
      image: mailhog/mailhog:latest
      imageID: docker-pullable://mailhog/mailhog@sha256:8d76a3d4ffa32a3661311944007a415332c4bb855657f4f6c57996405c009bea
      lastState: {}
      name: mailhog
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.112
    podIPs:
    - ip: 192.168.194.112
    - ip: fd07:b51a:cc66:a::ed
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: db42e6af1a032cfdcfd2bad943a75a924fe77785daf0a778e75e929dae1093c7
    creationTimestamp: "2025-01-24T06:28:34Z"
    generateName: prometheus-alertmanager-
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-alertmanager-6565764849
      statefulset.kubernetes.io/pod-name: prometheus-alertmanager-0
    name: prometheus-alertmanager-0
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-alertmanager
      uid: 0d13dc8f-ddda-4dc7-82c9-ad08f5c52528
    resourceVersion: "12571"
    uid: 3834278b-0ceb-426a-8c7e-f06963c683b2
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --storage.path=/alertmanager
      - --config.file=/etc/alertmanager/alertmanager.yml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.27.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/alertmanager
        name: config
      - mountPath: /alertmanager
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rsqhj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-alertmanager-0
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-alertmanager
    serviceAccountName: prometheus-alertmanager
    subdomain: prometheus-alertmanager-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-prometheus-alertmanager-0
    - configMap:
        defaultMode: 420
        name: prometheus-alertmanager
      name: config
    - name: kube-api-access-rsqhj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://68f07f350f27e0ecd11632e87c113fc5740fa38d48730c95c3f0dc64448f243a
      image: quay.io/prometheus/alertmanager:v0.27.0
      imageID: docker-pullable://quay.io/prometheus/alertmanager@sha256:e13b6ed5cb929eeaee733479dce55e10eb3bc2e9c4586c705a4e8da41e5eacf5
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:35Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.124
    podIPs:
    - ip: 192.168.194.124
    - ip: fd07:b51a:cc66:a::f9
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:32Z"
    generateName: prometheus-kube-state-metrics-8ff4967cb-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
      pod-template-hash: 8ff4967cb
    name: prometheus-kube-state-metrics-8ff4967cb-q5pf2
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-state-metrics-8ff4967cb
      uid: 3335467a-7012-47d0-8e02-42d59edb46b6
    resourceVersion: "12675"
    uid: e24fff0b-2f7d-4051-8c3c-cc1bef9e44b5
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-82mjb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-state-metrics
    serviceAccountName: prometheus-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-82mjb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://47bef47bdc1a008f55dbdba2fddbcd3b87835a1d4289a799c555392a3cb598ef
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imageID: docker-pullable://registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:37d841299325c23b56e5951176ce8ef317d537447c0f1b2d2437dddbb1f51165
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.114
    podIPs:
    - ip: 192.168.194.114
    - ip: fd07:b51a:cc66:a::ef
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-01-24T06:28:33Z"
    generateName: prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 5f44c7b88f
      helm.sh/chart: prometheus-node-exporter-4.43.1
      pod-template-generation: "1"
    name: prometheus-prometheus-node-exporter-cmq4q
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 422840fe-2a0f-454d-a628-16454faa3029
    resourceVersion: "12545"
    uid: f02f51ee-5c60-46ac-945d-d1b1fe164d69
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - orbstack
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: orbstack
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://49488ae67265cd8981f0c980f3e1a69bc20f7bdd4c39098f0f238a2bc2e67ddb
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: docker-pullable://quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:33Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 198.19.249.2
    podIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:32Z"
    generateName: prometheus-prometheus-pushgateway-7d7b6497cd-
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.10.0
      helm.sh/chart: prometheus-pushgateway-2.16.0
      pod-template-hash: 7d7b6497cd
    name: prometheus-prometheus-pushgateway-7d7b6497cd-bzq9x
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-prometheus-pushgateway-7d7b6497cd
      uid: 981edccf-70d4-4035-9b85-ecf24d3d3430
    resourceVersion: "12731"
    uid: 006db5f3-f7eb-4511-99bf-0b70fc53b48a
  spec:
    automountServiceAccountToken: true
    containers:
    - image: quay.io/prometheus/pushgateway:v1.10.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: pushgateway
      ports:
      - containerPort: 9091
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rff92
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-pushgateway
    serviceAccountName: prometheus-prometheus-pushgateway
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: storage-volume
    - name: kube-api-access-rff92
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9029667ec676b844a3967d973e0e02b05022a6770d041701b92664eb92b00eab
      image: quay.io/prometheus/pushgateway:v1.10.0
      imageID: docker-pullable://quay.io/prometheus/pushgateway@sha256:7a4d0696a24ef4e8bad62bee5656855a0aff2f26416d8cb32009dc28d6263604
      lastState: {}
      name: pushgateway
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.120
    podIPs:
    - ip: 192.168.194.120
    - ip: fd07:b51a:cc66:a::f6
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-01-24T06:28:32Z"
    generateName: prometheus-server-795bb7fcb4-
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.1.0
      pod-template-hash: 795bb7fcb4
    name: prometheus-server-795bb7fcb4-j9lbg
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-server-795bb7fcb4
      uid: 5d2aeaa6-93cf-441a-b85a-e0a5cd6b4d5a
    resourceVersion: "12777"
    uid: 936d06f9-3ae0-46e6-b087-d89e915cb3ac
  spec:
    containers:
    - args:
      - --watched-dir=/etc/config
      - --listen-address=0.0.0.0:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: prometheus-server-configmap-reload
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x4rjm
        readOnly: true
    - args:
      - --storage.tsdb.retention.time=15d
      - --config.file=/etc/config/prometheus.yml
      - --storage.tsdb.path=/data
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      image: quay.io/prometheus/prometheus:v3.1.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 10
      name: prometheus-server
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x4rjm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: orbstack
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-server
    serviceAccountName: prometheus-server
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-server
      name: config-volume
    - name: storage-volume
      persistentVolumeClaim:
        claimName: prometheus-server
    - name: kube-api-access-x4rjm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:29:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-01-24T06:28:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://265aec90987aa43180b4e716e893a9b7c7d8836d4bb0b49d0f0679d67ffcf677
      image: prom/prometheus:latest
      imageID: docker-pullable://prom/prometheus@sha256:6559acbd5d770b15bb3c954629ce190ac3cbbdb2b7f1c30f0385c4e05104e218
      lastState: {}
      name: prometheus-server
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    - containerID: docker://5bb731ce61ed724d514cea0d8716c1282bbfe044bda271ddfa85e1600f529163
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.2
      imageID: docker-pullable://quay.io/prometheus-operator/prometheus-config-reloader@sha256:944b2c67345c2dd9fafc4cddbf389cb09f930f9e83c8d06e90147076223a9e56
      lastState: {}
      name: prometheus-server-configmap-reload
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-01-24T06:28:34Z"
    hostIP: 198.19.249.2
    hostIPs:
    - ip: 198.19.249.2
    - ip: fd07:b51a:cc66::2
    phase: Running
    podIP: 192.168.194.113
    podIPs:
    - ip: 192.168.194.113
    - ip: fd07:b51a:cc66:a::ee
    qosClass: BestEffort
    startTime: "2025-01-24T06:28:32Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/version: 24.10.2
      helm.sh/chart: clickhouse-6.3.3
    name: clickhouse
    namespace: hyperswitch
    resourceVersion: "756"
    uid: e670ad3c-988f-4d5d-bbef-99b43353e77d
  spec:
    clusterIP: 192.168.194.215
    clusterIPs:
    - 192.168.194.215
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8123
      protocol: TCP
      targetPort: http
    - name: tcp
      port: 9000
      protocol: TCP
      targetPort: tcp
    - name: tcp-mysql
      port: 9004
      protocol: TCP
      targetPort: tcp-mysql
    - name: tcp-postgresql
      port: 9005
      protocol: TCP
      targetPort: tcp-postgresql
    - name: http-intersrv
      port: 9009
      protocol: TCP
      targetPort: http-intersrv
    selector:
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: clickhouse
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/version: 24.10.2
      helm.sh/chart: clickhouse-6.3.3
    name: clickhouse-headless
    namespace: hyperswitch
    resourceVersion: "695"
    uid: 8a508248-51f9-4b72-b875-131991f64825
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8123
      protocol: TCP
      targetPort: http
    - name: tcp
      port: 9000
      protocol: TCP
      targetPort: tcp
    - name: tcp-mysql
      port: 9004
      protocol: TCP
      targetPort: tcp-mysql
    - name: tcp-postgresql
      port: 9005
      protocol: TCP
      targetPort: tcp-postgresql
    - name: http-intersrv
      port: 9009
      protocol: TCP
      targetPort: http-intersrv
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: clickhouse
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-control-center
    namespace: hyperswitch
    resourceVersion: "772"
    uid: 34a13a1f-8fbe-47f5-b72f-7af1c8c23704
  spec:
    clusterIP: 192.168.194.211
    clusterIPs:
    - 192.168.194.211
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9000
    - name: https
      port: 443
      protocol: TCP
      targetPort: 9000
    selector:
      app: hyperswitch-control-center
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-server
    namespace: hyperswitch
    resourceVersion: "746"
    uid: 2caf4bc2-751f-4c7d-85a3-1500b80dd21d
  spec:
    clusterIP: 192.168.194.203
    clusterIPs:
    - 192.168.194.203
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: https
      port: 443
      protocol: TCP
      targetPort: 8080
    selector:
      app: hyperswitch-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.0.1
      helm.sh/chart: grafana-6.43.5
    name: hyperswitch-v1-grafana
    namespace: hyperswitch
    resourceVersion: "719"
    uid: 9f3323d2-e42f-4660-937a-eda318d6d927
  spec:
    clusterIP: 192.168.194.245
    clusterIPs:
    - 192.168.194.245
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: locker-db
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-locker-db
    namespace: hyperswitch
    resourceVersion: "732"
    uid: 7e7d7774-76b2-400b-99a7-df9e17557cbb
  spec:
    clusterIP: 192.168.194.189
    clusterIPs:
    - 192.168.194.189
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: locker-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: locker-db
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-locker-db-hl
    namespace: hyperswitch
    resourceVersion: "694"
    uid: 8f740bdc-db5f-4a78-a152-27a6f9bb5cb5
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: locker-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-postgresql
    namespace: hyperswitch
    resourceVersion: "766"
    uid: 1f069e13-9ce5-45e6-ba45-79cc5dd8a0c0
  spec:
    clusterIP: 192.168.194.208
    clusterIPs:
    - 192.168.194.208
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-postgresql-hl
    namespace: hyperswitch
    resourceVersion: "690"
    uid: 095f03b6-a942-42f8-a3da-72f9bf2add78
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-postgresql-read
    namespace: hyperswitch
    resourceVersion: "770"
    uid: 10ddba21-78db-4f23-b070-d7cbb6322051
  spec:
    clusterIP: 192.168.194.150
    clusterIPs:
    - 192.168.194.150
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-postgresql-read-hl
    namespace: hyperswitch
    resourceVersion: "698"
    uid: 56a18240-b974-4c20-a0e9-eab6644ff0a9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.3
      helm.sh/chart: redis-18.6.1
    name: hyperswitch-v1-redis-headless
    namespace: hyperswitch
    resourceVersion: "697"
    uid: 827629eb-36be-40f6-b72d-db1799b11b5a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.3
      helm.sh/chart: redis-18.6.1
    name: hyperswitch-v1-redis-master
    namespace: hyperswitch
    resourceVersion: "776"
    uid: a87112d6-1e6e-4308-bcde-312ce536599c
  spec:
    clusterIP: 192.168.194.153
    clusterIPs:
    - 192.168.194.153
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.3
      helm.sh/chart: redis-18.6.1
    name: hyperswitch-v1-redis-replicas
    namespace: hyperswitch
    resourceVersion: "774"
    uid: fd6d3a0d-11ca-4ab3-b8ba-c4891f766a7f
  spec:
    clusterIP: 192.168.194.218
    clusterIPs:
    - 192.168.194.218
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-redis
      port: 6379
      protocol: TCP
      targetPort: redis
    selector:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: Aggregator
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: vector
      app.kubernetes.io/version: 0.42.0-distroless-libc
      helm.sh/chart: vector-0.37.0
    name: hyperswitch-v1-vector
    namespace: hyperswitch
    resourceVersion: "744"
    uid: 3023463e-28f0-4d09-97af-527fa010785b
  spec:
    clusterIP: 192.168.194.202
    clusterIPs:
    - 192.168.194.202
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: api
      port: 8686
      protocol: TCP
      targetPort: 8686
    - name: sdk-source
      port: 3103
      protocol: TCP
      targetPort: 3103
    selector:
      app.kubernetes.io/component: Aggregator
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: vector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: Aggregator
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: vector
      app.kubernetes.io/version: 0.42.0-distroless-libc
      helm.sh/chart: vector-0.37.0
    name: hyperswitch-v1-vector-headless
    namespace: hyperswitch
    resourceVersion: "696"
    uid: bab81866-cab9-4f49-9c92-1527dbcc101d
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: api
      port: 8686
      protocol: TCP
      targetPort: 8686
    - name: sdk-source
      port: 3103
      protocol: TCP
      targetPort: 3103
    selector:
      app.kubernetes.io/component: Aggregator
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: vector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: zookeeper
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/version: 3.9.3
      helm.sh/chart: zookeeper-13.6.0
    name: hyperswitch-v1-zookeeper
    namespace: hyperswitch
    resourceVersion: "758"
    uid: d0d9411c-2e5a-4efa-8401-78237c15ef48
  spec:
    clusterIP: 192.168.194.164
    clusterIPs:
    - 192.168.194.164
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-client
      port: 2181
      protocol: TCP
      targetPort: client
    selector:
      app.kubernetes.io/component: zookeeper
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: zookeeper
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: zookeeper
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/version: 3.9.3
      helm.sh/chart: zookeeper-13.6.0
    name: hyperswitch-v1-zookeeper-headless
    namespace: hyperswitch
    resourceVersion: "691"
    uid: ed6b9e12-4d52-474e-b366-30a09317c2c6
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-client
      port: 2181
      protocol: TCP
      targetPort: client
    - name: tcp-follower
      port: 2888
      protocol: TCP
      targetPort: follower
    - name: tcp-election
      port: 3888
      protocol: TCP
      targetPort: election
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: zookeeper
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: zookeeper
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-vault
    namespace: hyperswitch
    resourceVersion: "725"
    uid: 5b864722-fd16-412f-b444-02159cd145ec
  spec:
    clusterIP: 192.168.194.147
    clusterIPs:
    - 192.168.194.147
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8080
    - name: https
      port: 443
      protocol: TCP
      targetPort: 8080
    selector:
      app: hyperswitch-card-vault
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hyperswitch-web
      app.kubernetes.io/version: 0.15.8
      helm.sh/chart: hyperswitch-web-0.2.0
    name: hyperswitch-web
    namespace: hyperswitch
    resourceVersion: "699"
    uid: f6bb6390-2d51-44db-811f-272b16c26f24
  spec:
    clusterIP: 192.168.194.242
    clusterIPs:
    - 192.168.194.242
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9090
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: hyperswitch-web
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: kafka
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/version: 3.9.0
      helm.sh/chart: kafka-31.0.0
    name: kafka0
    namespace: hyperswitch
    resourceVersion: "755"
    uid: a3bc0b2a-2cde-45e0-9050-a7d48ae1ce97
  spec:
    clusterIP: 192.168.194.148
    clusterIPs:
    - 192.168.194.148
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-client
      port: 29092
      protocol: TCP
      targetPort: client
    selector:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: broker
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.9.0
      helm.sh/chart: kafka-31.0.0
    name: kafka0-broker-headless
    namespace: hyperswitch
    resourceVersion: "692"
    uid: d47ce744-47ea-4f24-adf1-8366cf13a40d
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 29092
      protocol: TCP
      targetPort: client
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: broker
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.9.0
      helm.sh/chart: kafka-31.0.0
    name: kafka0-controller-headless
    namespace: hyperswitch
    resourceVersion: "693"
    uid: a5f3274b-93c1-42f1-88f4-10f404375e91
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 29092
      protocol: TCP
      targetPort: client
    - name: tcp-controller
      port: 9093
      protocol: TCP
      targetPort: controller
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: hyperswitch-v1
    name: loki
    namespace: hyperswitch
    resourceVersion: "743"
    uid: d7011682-a9d8-46ce-897c-e4761deaec65
  spec:
    clusterIP: 192.168.194.169
    clusterIPs:
    - 192.168.194.169
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: hyperswitch-v1
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: hyperswitch-v1
      variant: headless
    name: loki-headless
    namespace: hyperswitch
    resourceVersion: "720"
    uid: d2c93446-8e8e-428d-95de-6b9db12919ac
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: hyperswitch-v1
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: hyperswitch-v1
    name: loki-memberlist
    namespace: hyperswitch
    resourceVersion: "689"
    uid: f3b27fc8-5d43-4862-96b4-1358149d246a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7946
      protocol: TCP
      targetPort: memberlist-port
    publishNotReadyAddresses: true
    selector:
      app: loki
      release: hyperswitch-v1
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mailhog
      app.kubernetes.io/version: v1.0.1
      helm.sh/chart: mailhog-4.0.0
    name: mailhog
    namespace: hyperswitch
    resourceVersion: "735"
    uid: e185b7b3-9957-46ee-aed3-56bba10d37aa
  spec:
    clusterIP: 192.168.194.190
    clusterIPs:
    - 192.168.194.190
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8025
      protocol: TCP
      targetPort: http
    - name: tcp-smtp
      port: 1025
      protocol: TCP
      targetPort: tcp-smtp
    selector:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: mailhog
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.27.0
      helm.sh/chart: alertmanager-1.13.1
    name: prometheus-alertmanager
    namespace: hyperswitch
    resourceVersion: "5285"
    uid: 4fa6df65-3031-455b-bff6-bae59891f890
  spec:
    clusterIP: 192.168.194.216
    clusterIPs:
    - 192.168.194.216
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9093
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.27.0
      helm.sh/chart: alertmanager-1.13.1
    name: prometheus-alertmanager-headless
    namespace: hyperswitch
    resourceVersion: "5281"
    uid: 1a7f7bb8-a65f-431d-a7ce-30afea650c4c
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9093
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-01-19T18:44:31Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
    name: prometheus-kube-state-metrics
    namespace: hyperswitch
    resourceVersion: "5302"
    uid: 5d78da3b-3ccd-4e2e-a3fa-5c5f844798bf
  spec:
    clusterIP: 192.168.194.204
    clusterIPs:
    - 192.168.194.204
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-01-19T18:44:31Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.43.1
    name: prometheus-prometheus-node-exporter
    namespace: hyperswitch
    resourceVersion: "5296"
    uid: 8d12a0a4-7661-450f-b9ff-a07b32c6cb80
  spec:
    clusterIP: 192.168.194.232
    clusterIPs:
    - 192.168.194.232
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
      prometheus.io/probe: pushgateway
    creationTimestamp: "2025-01-19T18:44:31Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.10.0
      helm.sh/chart: prometheus-pushgateway-2.16.0
    name: prometheus-prometheus-pushgateway
    namespace: hyperswitch
    resourceVersion: "5290"
    uid: faf051c3-7cc3-4cc2-868f-8a40757805e9
  spec:
    clusterIP: 192.168.194.154
    clusterIPs:
    - 192.168.194.154
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9091
      protocol: TCP
      targetPort: 9091
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-pushgateway
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.1.0
    name: prometheus-server
    namespace: hyperswitch
    resourceVersion: "5292"
    uid: b0e913b5-84f3-446f-af27-c9ef4b4f8e8c
  spec:
    clusterIP: 192.168.194.157
    clusterIPs:
    - 192.168.194.157
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 2.9.3
      helm.sh/chart: promtail-6.15.5
    name: hyperswitch-v1-promtail
    namespace: hyperswitch
    resourceVersion: "12822"
    uid: 81994c17-2e6a-4b66-8da6-e7aaf9e96613
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: promtail
    template:
      metadata:
        annotations:
          checksum/config: 30f5c7b874124b12826873762af93f265f37fff1f305930c16692b43d107e707
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: promtail
      spec:
        containers:
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/promtail:2.9.3
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 3101
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: config
          - mountPath: /run/promtail
            name: run
          - mountPath: /var/lib/docker/containers
            name: containers
            readOnly: true
          - mountPath: /var/log/pods
            name: pods
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 0
          runAsUser: 0
        serviceAccount: hyperswitch-v1-promtail
        serviceAccountName: hyperswitch-v1-promtail
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: hyperswitch-v1-promtail
        - hostPath:
            path: /run/promtail
            type: ""
          name: run
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: containers
        - hostPath:
            path: /var/log/pods
            type: ""
          name: pods
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberMisscheduled: 0
    numberReady: 0
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.43.1
    name: prometheus-prometheus-node-exporter
    namespace: hyperswitch
    resourceVersion: "12548"
    uid: 422840fe-2a0f-454d-a628-16454faa3029
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.8.2
          helm.sh/chart: prometheus-node-exporter-4.43.1
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.8.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-node-exporter
        serviceAccountName: prometheus-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-card-vault-hyperswitch-v1
    namespace: hyperswitch
    resourceVersion: "12791"
    uid: 1cecb08a-0fb8-4415-8d39-0cbef0816666
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: hyperswitch-card-vault
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: a59f3a529ee91abf28ef1173d6949da391b5a2d4449a31561311d3fb3e2049c1
          checksum/secrets: 7c50797f771bff2ddd398a26e01d3cf5da19b74b83af78d45b755eed439cdccc
        creationTimestamp: null
        labels:
          app: hyperswitch-card-vault
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - env:
          - name: LOCKER__LOG__CONSOLE__ENABLED
            value: "true"
          - name: LOCKER__LOG__CONSOLE__LEVEL
            value: DEBUG
          - name: LOCKER__LOG__CONSOLE__LOG_FORMAT
            value: default
          - name: LOCKER__DATABASE__USERNAME
            value: db_user
          - name: LOCKER__DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: LOCKER__DATABASE__PASSWORD
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__DATABASE__HOST
            value: hyperswitch-v1-locker-db
          - name: LOCKER__DATABASE__PORT
            value: "5432"
          - name: LOCKER__DATABASE__DBNAME
            value: locker-db
          - name: LOCKER__LIMIT__REQUEST_COUNT
            value: "100"
          - name: LOCKER__LIMIT__DURATION
            value: "60"
          - name: LOCKER__SECRETS__TENANT
            value: hyperswitch
          - name: LOCKER__SECRETS__MASTER_KEY
            valueFrom:
              secretKeyRef:
                key: LOCKER__SECRETS__MASTER_KEY
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__SECRETS__TENANT_PUBLIC_KEY
            valueFrom:
              secretKeyRef:
                key: LOCKER__SECRETS__TENANT_PUBLIC_KEY
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__CACHE__MAX_CAPACITY
            value: "5000"
          - name: LOCKER__CACHE__TTI
            value: "7200"
          image: juspaydotin/hyperswitch-card-vault:v0.4.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: tartarus
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 50
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/development.toml
            name: hyperswitch-vault-config
            subPath: development.toml
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-vault-role
        serviceAccountName: hyperswitch-vault-role
        terminationGracePeriodSeconds: 120
        volumes:
        - configMap:
            defaultMode: 420
            name: hyperswitch-vault-config-hyperswitch-v1
          name: hyperswitch-vault-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:16:55Z"
      message: ReplicaSet "hyperswitch-card-vault-hyperswitch-v1-55b446ccdb" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:29:22Z"
      lastUpdateTime: "2025-01-24T06:29:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-consumer-v1o112o0
    namespace: hyperswitch
    resourceVersion: "12767"
    uid: 197d1ab4-ae5a-4fad-bc37-7dbfa3455419
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: hyperswitch-consumer
        version: consumer-v1o112o0
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/consumer-config: 1a52e188a99d5e8019f95e8697727b13673a9a02b2c018f027256562314d7718
          checksum/consumer-secret: 7675a2e818b767925d905355e34ccd18ec440343318e2121f7c593084bcc0414
          checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
          kubectl.kubernetes.io/restartedAt: "2023-04-21T14:21:23+05:30"
        creationTimestamp: null
        labels:
          app: hyperswitch-consumer
          version: consumer-v1o112o0
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
                name: hyperswitch-secrets
          - name: ROUTER__KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__ADMIN_API_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__ADMIN_API_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__JWT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__JWT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__MASTER_ENC_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__MASTER_ENC_KEY
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SENDER_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SENDER_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
            value: "1"
          - name: ROUTER__EMAIL__AWS_REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_REGION
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__PORT
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PORT
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__USERNAME
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__SMTP__PASSWORD
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PASSWORD
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
                name: hyperswitch-secrets
          - name: RUN_ENV
            value: sandbox
          - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: clickhouse
          - name: ROUTER__ANALYTICS__SQLX__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__MASTER_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__REPLICA_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          envFrom:
          - secretRef:
              name: consumer-secret-hyperswitch-v1
          - secretRef:
              name: hyperswitch-secrets
          image: juspaydotin/hyperswitch-consumer:v1.112.1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          name: hyperswitch-consumer
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/sandbox.toml
            name: hyperswitch-config
            subPath: consumer.toml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "PostgreSQL did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "PostgreSQL is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          name: check-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "Redis did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for Redis to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "Redis is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          name: check-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: consumer-cm-hyperswitch-v1
          name: hyperswitch-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:18:04Z"
      message: ReplicaSet "hyperswitch-consumer-v1o112o0-785f675ddc" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:29:01Z"
      lastUpdateTime: "2025-01-24T06:29:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-control-center-v1o34o2
    namespace: hyperswitch
    resourceVersion: "12440"
    uid: 55f1189a-6ba2-420d-ad9d-1fa630454720
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: hyperswitch-control-center
        version: v1o34o2
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: hyperswitch-control-center
          version: v1o34o2
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: binary
            value: dashboard
          - name: default__endpoints__agreement_url
            value: https://app.hyperswitch.io/agreement/tc-hyperswitch-aug-23.pdf
          - name: default__endpoints__agreement_version
            value: 1.0.0
          - name: default__endpoints__dss_certificate_url
            value: https://app.hyperswitch.io/certificates/PCI_DSS_v4-0_AOC_Juspay_2024.pdf
          - name: default__endpoints__favicon_url
          - name: default__endpoints__logo_url
          - name: default__endpoints__mixpanel_token
            value: dd4da7f62941557e716fbc0a19f9cc7e
          - name: default__features__authentication_analytics
            value: "false"
          - name: default__features__branding
            value: "false"
          - name: default__features__compliance_certificate
            value: "false"
          - name: default__features__configure_pmts
            value: "false"
          - name: default__features__custom_webhook_headers
            value: "false"
          - name: default__features__dispute_analytics
            value: "false"
          - name: default__features__dispute_evidence_upload
            value: "false"
          - name: default__features__email
            value: "true"
          - name: default__features__feedback
            value: "false"
          - name: default__features__frm
            value: "false"
          - name: default__features__generate_report
            value: "false"
          - name: default__features__global_search
            value: "false"
          - name: default__features__is_live_mode
            value: "false"
          - name: default__features__live_users_counter
            value: "false"
          - name: default__features__mixpanel
            value: "false"
          - name: default__features__payout
            value: "false"
          - name: default__features__paypal_automatic_flow
            value: "false"
          - name: default__features__performance_monitor
            value: "false"
          - name: default__features__pm_authentication_processor
            value: "false"
          - name: default__features__quick_start
            value: "false"
          - name: default__features__recon
            value: "false"
          - name: default__features__sample_data
            value: "false"
          - name: default__features__surcharge
            value: "false"
          - name: default__features__system_metrics
            value: "false"
          - name: default__features__test_live_toggle
            value: "false"
          - name: default__features__test_processors
            value: "true"
          - name: default__features__threeds_authenticator
            value: "false"
          - name: default__features__totp
            value: "true"
          - name: default__features__user_journey_analytics
            value: "false"
          - name: default__theme__primary_color
            value: '#006DF9'
          - name: default__theme__primary_hover_color
            value: '#005ED6'
          - name: default__theme__sidebar_color
            value: '#242F48'
          - name: host
            value: hyperswitch-control-center
          - name: mixpanelToken
            value: dd4da7f62941557e716fbc0a19f9cc7e
          - name: apiBaseUrl
            value: http://localhost:8080
          - name: sdkBaseUrl
            value: http://localhost:9090/web/0.103.1/v0/HyperLoader.js
          - name: default__endpoints__api_url
            value: http://localhost:8080
          - name: default__endpoints__sdk_url
            value: http://localhost:9090/web/0.103.1/v0/HyperLoader.js
          - name: default__endpoints__apple_pay_certificate_url
            value: http://localhost:8080/applepay-domain/apple-developer-merchantid-domain-association
          - name: default__features__audit_trail
            value: "true"
          image: juspaydotin/hyperswitch-control-center:v1.36.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          name: hyperswitch-control-center
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:16:30Z"
      message: ReplicaSet "hyperswitch-control-center-v1o34o2-69b889f7d8" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:28:34Z"
      lastUpdateTime: "2025-01-24T06:28:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-producer-v1o112o0
    namespace: hyperswitch
    resourceVersion: "12770"
    uid: 836de44c-a50e-4ec5-8c4e-faa03f3911c3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: hyperswitch-producer
        version: v1o112o0
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
          checksum/producer-config: c26788fc47ffb9d08f324bf680a4c6bdbbff6c7c16a7283b6125251c08cb28b6
          checksum/producer-secret: 684916708382dcdd4efa45fcc8c13bd5b8195d9554834fd29daa7382659e0e52
        creationTimestamp: null
        labels:
          app: hyperswitch-producer
          version: v1o112o0
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
                name: hyperswitch-secrets
          - name: ROUTER__KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__ADMIN_API_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__ADMIN_API_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__JWT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__JWT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__MASTER_ENC_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__MASTER_ENC_KEY
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SENDER_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SENDER_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
            value: "1"
          - name: ROUTER__EMAIL__AWS_REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_REGION
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__PORT
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PORT
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__USERNAME
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__SMTP__PASSWORD
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PASSWORD
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
                name: hyperswitch-secrets
          - name: RUN_ENV
            value: sandbox
          - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: clickhouse
          - name: ROUTER__ANALYTICS__SQLX__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__MASTER_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__REPLICA_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          envFrom:
          - secretRef:
              name: producer-secret-hyperswitch-v1
          - secretRef:
              name: hyperswitch-secrets
          image: juspaydotin/hyperswitch-producer:v1.112.1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          name: hyperswitch-producer
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/sandbox.toml
            name: hyperswitch-config
            subPath: producer.toml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "PostgreSQL did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "PostgreSQL is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          name: check-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "Redis did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for Redis to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "Redis is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          name: check-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: producer-cm-hyperswitch-v1
          name: hyperswitch-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:18:02Z"
      message: ReplicaSet "hyperswitch-producer-v1o112o0-7d88bff947" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:29:01Z"
      lastUpdateTime: "2025-01-24T06:29:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: hyperswitch-server-v1o112o0
    namespace: hyperswitch
    resourceVersion: "12802"
    uid: 8c35546d-ddd5-49fe-9327-ed4da34a9e85
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: hyperswitch-server
        version: v1o112o0
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
          checksum/router-config: c17d39eed43a8eb1b1b6406f54b5596ffa6eb07993ad56c075c8c5e7e51ff2fc
          checksum/router-secret: 09bf14ce697665acaf53ee21bda701cd770270810e0b0ba50f219491a3f1ac5c
          kubectl.kubernetes.io/restartedAt: "2023-09-20T12:11:41+05:30"
          traffic.sidecar.istio.io/excludeOutboundIPRanges: 10.23.6.12/32
        creationTimestamp: null
        labels:
          app: hyperswitch-server
          version: v1o112o0
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: BINARY
            value: router
          - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
                name: hyperswitch-secrets
          - name: ROUTER__KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__ADMIN_API_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__ADMIN_API_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__JWT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__JWT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__MASTER_ENC_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__MASTER_ENC_KEY
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SENDER_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SENDER_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
            value: "1"
          - name: ROUTER__EMAIL__AWS_REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_REGION
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__PORT
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PORT
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__USERNAME
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__SMTP__PASSWORD
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PASSWORD
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
                name: hyperswitch-secrets
          - name: RUN_ENV
            value: sandbox
          - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: clickhouse
          - name: ROUTER__ANALYTICS__SQLX__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__MASTER_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__REPLICA_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          envFrom:
          - configMapRef:
              name: router-cm-hyperswitch-v1
          - secretRef:
              name: router-secret-hyperswitch-v1
          - secretRef:
              name: hyperswitch-secrets
          image: juspaydotin/hyperswitch-router:v1.112.1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 1
          name: hyperswitch-router
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 200m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/sandbox.toml
            name: hyperswitch-config
            subPath: router.toml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "PostgreSQL did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "PostgreSQL is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          name: check-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "Redis did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for Redis to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "Redis is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          name: check-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: router-cm-hyperswitch-v1
          name: hyperswitch-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:18:05Z"
      message: ReplicaSet "hyperswitch-server-v1o112o0-867fc9666f" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:29:32Z"
      lastUpdateTime: "2025-01-24T06:29:32Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 10.0.1
      helm.sh/chart: grafana-6.43.5
    name: hyperswitch-v1-grafana
    namespace: hyperswitch
    resourceVersion: "12674"
    uid: ea5853a5-af48-41e7-88c9-ace5c94d937c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 322df4b327f90632afc999b8103eaa8db5c5b439456313837a9f8e936616cfec
          checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
          checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
          checksum/secret: 41fa47582aada96cfd80f98d18f43725edec86c8243f67111c5621507e907350
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: grafana
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: hyperswitch-v1-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: hyperswitch-v1-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.19.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: hyperswitch-v1-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: hyperswitch-v1-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: grafana/grafana:10.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsUser: 472
        serviceAccount: hyperswitch-v1-grafana
        serviceAccountName: hyperswitch-v1-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: hyperswitch-v1-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:17:28Z"
      message: ReplicaSet "hyperswitch-v1-grafana-5c95d79b6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:28:42Z"
      lastUpdateTime: "2025-01-24T06:28:42Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hyperswitch-web
      app.kubernetes.io/version: 0.15.8
      helm.sh/chart: hyperswitch-web-0.2.0
    name: hyperswitch-web
    namespace: hyperswitch
    resourceVersion: "12468"
    uid: 53c7a49f-bc97-4d2b-bc6b-c3daaf868e09
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: hyperswitch-web
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          helm.sh/hook: post-install,post-upgrade
          helm.sh/hook-delete-policy: hook-succeeded
          helm.sh/hook-weight: "-5"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: hyperswitch-web
          app.kubernetes.io/version: 0.15.8
          helm.sh/chart: hyperswitch-web-0.2.0
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: hyperswitch-web-nginx
          image: juspaydotin/hyperswitch-web:0.103.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /web/0.103.1/v0/
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: hyperswitch-web
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /web/0.103.1/v0/
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 1500m
              memory: 3Gi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: nginx-config-volume
            subPath: default.conf
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-web
        serviceAccountName: hyperswitch-web
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: hyperswitch-web-nginx
          name: nginx-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:16:23Z"
      message: ReplicaSet "hyperswitch-web-5475d55478" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:28:34Z"
      lastUpdateTime: "2025-01-24T06:28:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mailhog
      app.kubernetes.io/version: v1.0.1
      helm.sh/chart: mailhog-4.0.0
    name: mailhog
    namespace: hyperswitch
    resourceVersion: "12477"
    uid: a54e03b9-882c-48ae-9bc7-f38a98fc1ace
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: mailhog
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: mailhog
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: MH_HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/mailhog/mailhog:v1.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: tcp-smtp
            timeoutSeconds: 1
          name: mailhog
          ports:
          - containerPort: 8025
            name: http
            protocol: TCP
          - containerPort: 1025
            name: tcp-smtp
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: tcp-smtp
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: mailhog
        serviceAccountName: mailhog
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-16T10:16:05Z"
      lastUpdateTime: "2025-01-16T10:16:20Z"
      message: ReplicaSet "mailhog-765485df76" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:28:34Z"
      lastUpdateTime: "2025-01-24T06:28:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
    name: prometheus-kube-state-metrics
    namespace: hyperswitch
    resourceVersion: "12679"
    uid: 41c943d7-6109-4dd7-ac6d-e838bf7defc8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.0
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-19T18:44:31Z"
      lastUpdateTime: "2025-01-19T18:44:52Z"
      message: ReplicaSet "prometheus-kube-state-metrics-8ff4967cb" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:28:43Z"
      lastUpdateTime: "2025-01-24T06:28:43Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.10.0
      helm.sh/chart: prometheus-pushgateway-2.16.0
    name: prometheus-prometheus-pushgateway
    namespace: hyperswitch
    resourceVersion: "12735"
    uid: 518d3c97-33e1-4643-9a1f-f07aa1dd9082
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.10.0
          helm.sh/chart: prometheus-pushgateway-2.16.0
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.10.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-19T18:44:31Z"
      lastUpdateTime: "2025-01-19T18:45:02Z"
      message: ReplicaSet "prometheus-prometheus-pushgateway-7d7b6497cd" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:28:53Z"
      lastUpdateTime: "2025-01-24T06:28:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.1.0
    name: prometheus-server
    namespace: hyperswitch
    resourceVersion: "12781"
    uid: 537d3431-3699-4471-bd9e-72ac0bb4cf34
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.1.0
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-01-19T18:44:31Z"
      lastUpdateTime: "2025-01-19T18:45:46Z"
      message: ReplicaSet "prometheus-server-795bb7fcb4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-01-24T06:29:08Z"
      lastUpdateTime: "2025-01-24T06:29:08Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app: hyperswitch-card-vault
      pod-template-hash: 55b446ccdb
    name: hyperswitch-card-vault-hyperswitch-v1-55b446ccdb
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hyperswitch-card-vault-hyperswitch-v1
      uid: 1cecb08a-0fb8-4415-8d39-0cbef0816666
    resourceVersion: "12789"
    uid: 4193f8c8-a9c0-490c-8f46-69c8eaf23671
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hyperswitch-card-vault
        pod-template-hash: 55b446ccdb
    template:
      metadata:
        annotations:
          checksum/config: a59f3a529ee91abf28ef1173d6949da391b5a2d4449a31561311d3fb3e2049c1
          checksum/secrets: 7c50797f771bff2ddd398a26e01d3cf5da19b74b83af78d45b755eed439cdccc
        creationTimestamp: null
        labels:
          app: hyperswitch-card-vault
          pod-template-hash: 55b446ccdb
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - env:
          - name: LOCKER__LOG__CONSOLE__ENABLED
            value: "true"
          - name: LOCKER__LOG__CONSOLE__LEVEL
            value: DEBUG
          - name: LOCKER__LOG__CONSOLE__LOG_FORMAT
            value: default
          - name: LOCKER__DATABASE__USERNAME
            value: db_user
          - name: LOCKER__DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: LOCKER__DATABASE__PASSWORD
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__DATABASE__HOST
            value: hyperswitch-v1-locker-db
          - name: LOCKER__DATABASE__PORT
            value: "5432"
          - name: LOCKER__DATABASE__DBNAME
            value: locker-db
          - name: LOCKER__LIMIT__REQUEST_COUNT
            value: "100"
          - name: LOCKER__LIMIT__DURATION
            value: "60"
          - name: LOCKER__SECRETS__TENANT
            value: hyperswitch
          - name: LOCKER__SECRETS__MASTER_KEY
            valueFrom:
              secretKeyRef:
                key: LOCKER__SECRETS__MASTER_KEY
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: LOCKER__SECRETS__LOCKER_PRIVATE_KEY
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__SECRETS__TENANT_PUBLIC_KEY
            valueFrom:
              secretKeyRef:
                key: LOCKER__SECRETS__TENANT_PUBLIC_KEY
                name: locker-secrets-hyperswitch-v1
          - name: LOCKER__CACHE__MAX_CAPACITY
            value: "5000"
          - name: LOCKER__CACHE__TTI
            value: "7200"
          image: juspaydotin/hyperswitch-card-vault:v0.4.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: tartarus
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 50
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/development.toml
            name: hyperswitch-vault-config
            subPath: development.toml
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-vault-role
        serviceAccountName: hyperswitch-vault-role
        terminationGracePeriodSeconds: 120
        volumes:
        - configMap:
            defaultMode: 420
            name: hyperswitch-vault-config-hyperswitch-v1
          name: hyperswitch-vault-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app: hyperswitch-consumer
      pod-template-hash: 785f675ddc
      version: consumer-v1o112o0
    name: hyperswitch-consumer-v1o112o0-785f675ddc
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hyperswitch-consumer-v1o112o0
      uid: 197d1ab4-ae5a-4fad-bc37-7dbfa3455419
    resourceVersion: "12765"
    uid: c9aeea99-7670-4837-93c9-365573803a8e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hyperswitch-consumer
        pod-template-hash: 785f675ddc
        version: consumer-v1o112o0
    template:
      metadata:
        annotations:
          checksum/consumer-config: 1a52e188a99d5e8019f95e8697727b13673a9a02b2c018f027256562314d7718
          checksum/consumer-secret: 7675a2e818b767925d905355e34ccd18ec440343318e2121f7c593084bcc0414
          checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
          kubectl.kubernetes.io/restartedAt: "2023-04-21T14:21:23+05:30"
        creationTimestamp: null
        labels:
          app: hyperswitch-consumer
          pod-template-hash: 785f675ddc
          version: consumer-v1o112o0
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
                name: hyperswitch-secrets
          - name: ROUTER__KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__ADMIN_API_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__ADMIN_API_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__JWT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__JWT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__MASTER_ENC_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__MASTER_ENC_KEY
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SENDER_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SENDER_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
            value: "1"
          - name: ROUTER__EMAIL__AWS_REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_REGION
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__PORT
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PORT
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__USERNAME
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__SMTP__PASSWORD
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PASSWORD
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
                name: hyperswitch-secrets
          - name: RUN_ENV
            value: sandbox
          - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: clickhouse
          - name: ROUTER__ANALYTICS__SQLX__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__MASTER_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__REPLICA_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          envFrom:
          - secretRef:
              name: consumer-secret-hyperswitch-v1
          - secretRef:
              name: hyperswitch-secrets
          image: juspaydotin/hyperswitch-consumer:v1.112.1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          name: hyperswitch-consumer
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/sandbox.toml
            name: hyperswitch-config
            subPath: consumer.toml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "PostgreSQL did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "PostgreSQL is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          name: check-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "Redis did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for Redis to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "Redis is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          name: check-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: consumer-cm-hyperswitch-v1
          name: hyperswitch-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app: hyperswitch-control-center
      pod-template-hash: 69b889f7d8
      version: v1o34o2
    name: hyperswitch-control-center-v1o34o2-69b889f7d8
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hyperswitch-control-center-v1o34o2
      uid: 55f1189a-6ba2-420d-ad9d-1fa630454720
    resourceVersion: "12434"
    uid: 13a1d4b8-fc8a-4d4c-9f5a-57ab4f723e2b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hyperswitch-control-center
        pod-template-hash: 69b889f7d8
        version: v1o34o2
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: hyperswitch-control-center
          pod-template-hash: 69b889f7d8
          version: v1o34o2
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: binary
            value: dashboard
          - name: default__endpoints__agreement_url
            value: https://app.hyperswitch.io/agreement/tc-hyperswitch-aug-23.pdf
          - name: default__endpoints__agreement_version
            value: 1.0.0
          - name: default__endpoints__dss_certificate_url
            value: https://app.hyperswitch.io/certificates/PCI_DSS_v4-0_AOC_Juspay_2024.pdf
          - name: default__endpoints__favicon_url
          - name: default__endpoints__logo_url
          - name: default__endpoints__mixpanel_token
            value: dd4da7f62941557e716fbc0a19f9cc7e
          - name: default__features__authentication_analytics
            value: "false"
          - name: default__features__branding
            value: "false"
          - name: default__features__compliance_certificate
            value: "false"
          - name: default__features__configure_pmts
            value: "false"
          - name: default__features__custom_webhook_headers
            value: "false"
          - name: default__features__dispute_analytics
            value: "false"
          - name: default__features__dispute_evidence_upload
            value: "false"
          - name: default__features__email
            value: "true"
          - name: default__features__feedback
            value: "false"
          - name: default__features__frm
            value: "false"
          - name: default__features__generate_report
            value: "false"
          - name: default__features__global_search
            value: "false"
          - name: default__features__is_live_mode
            value: "false"
          - name: default__features__live_users_counter
            value: "false"
          - name: default__features__mixpanel
            value: "false"
          - name: default__features__payout
            value: "false"
          - name: default__features__paypal_automatic_flow
            value: "false"
          - name: default__features__performance_monitor
            value: "false"
          - name: default__features__pm_authentication_processor
            value: "false"
          - name: default__features__quick_start
            value: "false"
          - name: default__features__recon
            value: "false"
          - name: default__features__sample_data
            value: "false"
          - name: default__features__surcharge
            value: "false"
          - name: default__features__system_metrics
            value: "false"
          - name: default__features__test_live_toggle
            value: "false"
          - name: default__features__test_processors
            value: "true"
          - name: default__features__threeds_authenticator
            value: "false"
          - name: default__features__totp
            value: "true"
          - name: default__features__user_journey_analytics
            value: "false"
          - name: default__theme__primary_color
            value: '#006DF9'
          - name: default__theme__primary_hover_color
            value: '#005ED6'
          - name: default__theme__sidebar_color
            value: '#242F48'
          - name: host
            value: hyperswitch-control-center
          - name: mixpanelToken
            value: dd4da7f62941557e716fbc0a19f9cc7e
          - name: apiBaseUrl
            value: http://localhost:8080
          - name: sdkBaseUrl
            value: http://localhost:9090/web/0.103.1/v0/HyperLoader.js
          - name: default__endpoints__api_url
            value: http://localhost:8080
          - name: default__endpoints__sdk_url
            value: http://localhost:9090/web/0.103.1/v0/HyperLoader.js
          - name: default__endpoints__apple_pay_certificate_url
            value: http://localhost:8080/applepay-domain/apple-developer-merchantid-domain-association
          - name: default__features__audit_trail
            value: "true"
          image: juspaydotin/hyperswitch-control-center:v1.36.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          name: hyperswitch-control-center
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app: hyperswitch-producer
      pod-template-hash: 7d88bff947
      version: v1o112o0
    name: hyperswitch-producer-v1o112o0-7d88bff947
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hyperswitch-producer-v1o112o0
      uid: 836de44c-a50e-4ec5-8c4e-faa03f3911c3
    resourceVersion: "12768"
    uid: ba133079-95c8-4bdd-8568-75ad40729497
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hyperswitch-producer
        pod-template-hash: 7d88bff947
        version: v1o112o0
    template:
      metadata:
        annotations:
          checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
          checksum/producer-config: c26788fc47ffb9d08f324bf680a4c6bdbbff6c7c16a7283b6125251c08cb28b6
          checksum/producer-secret: 684916708382dcdd4efa45fcc8c13bd5b8195d9554834fd29daa7382659e0e52
        creationTimestamp: null
        labels:
          app: hyperswitch-producer
          pod-template-hash: 7d88bff947
          version: v1o112o0
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
                name: hyperswitch-secrets
          - name: ROUTER__KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__ADMIN_API_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__ADMIN_API_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__JWT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__JWT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__MASTER_ENC_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__MASTER_ENC_KEY
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SENDER_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SENDER_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
            value: "1"
          - name: ROUTER__EMAIL__AWS_REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_REGION
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__PORT
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PORT
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__USERNAME
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__SMTP__PASSWORD
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PASSWORD
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
                name: hyperswitch-secrets
          - name: RUN_ENV
            value: sandbox
          - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: clickhouse
          - name: ROUTER__ANALYTICS__SQLX__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__MASTER_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__REPLICA_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          envFrom:
          - secretRef:
              name: producer-secret-hyperswitch-v1
          - secretRef:
              name: hyperswitch-secrets
          image: juspaydotin/hyperswitch-producer:v1.112.1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          name: hyperswitch-producer
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/sandbox.toml
            name: hyperswitch-config
            subPath: producer.toml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "PostgreSQL did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "PostgreSQL is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          name: check-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "Redis did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for Redis to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "Redis is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          name: check-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: producer-cm-hyperswitch-v1
          name: hyperswitch-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app: hyperswitch-server
      pod-template-hash: 867fc9666f
      version: v1o112o0
    name: hyperswitch-server-v1o112o0-867fc9666f
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hyperswitch-server-v1o112o0
      uid: 8c35546d-ddd5-49fe-9327-ed4da34a9e85
    resourceVersion: "12801"
    uid: c536ca94-a42d-4b5a-af30-c835ae013087
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hyperswitch-server
        pod-template-hash: 867fc9666f
        version: v1o112o0
    template:
      metadata:
        annotations:
          checksum/misc-secrets: f75dd95138354346af234ad4342aae68cfb2094e8d706bdd0f67308dab8a1ef7
          checksum/router-config: c17d39eed43a8eb1b1b6406f54b5596ffa6eb07993ad56c075c8c5e7e51ff2fc
          checksum/router-secret: 09bf14ce697665acaf53ee21bda701cd770270810e0b0ba50f219491a3f1ac5c
          kubectl.kubernetes.io/restartedAt: "2023-09-20T12:11:41+05:30"
          traffic.sidecar.istio.io/excludeOutboundIPRanges: 10.23.6.12/32
        creationTimestamp: null
        labels:
          app: hyperswitch-server
          pod-template-hash: 867fc9666f
          version: v1o112o0
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-type
                  operator: In
                  values:
                  - generic-compute
        containers:
        - env:
          - name: BINARY
            value: router
          - name: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__JWEKEY__TUNNEL_PRIVATE_KEY
                name: hyperswitch-secrets
          - name: ROUTER__KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__ADMIN_API_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__ADMIN_API_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__JWT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__JWT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__API_KEYS__KMS_ENCRYPTED_HASH_KEY
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS__MASTER_ENC_KEY
            valueFrom:
              secretKeyRef:
                key: ROUTER__SECRETS__MASTER_ENC_KEY
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_ID
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__CLIENT_SECRET
                name: hyperswitch-secrets
          - name: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__CONNECTOR_ONBOARDING__PAYPAL__PARTNER_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__SECRETS_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__KEY_ID
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__KEY_ID
                name: hyperswitch-secrets
          - name: ROUTER__ENCRYPTION_MANAGEMENT__AWS_KMS__REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__KMS__REGION
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__USERNAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__DATABASE_NAME
                name: hyperswitch-secrets
          - name: ROUTER__ANALYTICS__CLICKHOUSE__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__ANALYTICS__CLICKHOUSE__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SENDER_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SENDER_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__ALLOWED_UNVERIFIED_DAYS
            value: "1"
          - name: ROUTER__EMAIL__AWS_REGION
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_REGION
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__RECON_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__PROD_INTENT_RECIPIENT_EMAIL
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__HOST
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__HOST
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__PORT
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PORT
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__SMTP__USERNAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__USERNAME
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__SMTP__PASSWORD
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__SMTP__PASSWORD
                name: hyperswitch-secrets
                optional: true
          - name: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__EMAIL_ROLE_ARN
                name: hyperswitch-secrets
          - name: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
            valueFrom:
              secretKeyRef:
                key: ROUTER__EMAIL__AWS_SES__STS_ROLE_SESSION_NAME
                name: hyperswitch-secrets
          - name: RUN_ENV
            value: sandbox
          - name: ROUTER__ANALYTICS__CLICKHOUSE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: clickhouse
          - name: ROUTER__ANALYTICS__SQLX__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__MASTER_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: ROUTER__REPLICA_DATABASE__PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          envFrom:
          - configMapRef:
              name: router-cm-hyperswitch-v1
          - secretRef:
              name: router-secret-hyperswitch-v1
          - secretRef:
              name: hyperswitch-secrets
          image: juspaydotin/hyperswitch-router:v1.112.1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -15 node
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 1
          name: hyperswitch-router
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 200m
              memory: 500Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /local/config/sandbox.toml
            name: hyperswitch-config
            subPath: router.toml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! pg_isready -U hyperswitch -d hyperswitch -h hyperswitch-v1-postgresql -p 5432; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "PostgreSQL did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for PostgreSQL to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "PostgreSQL is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          name: check-postgres
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - args:
          - |
            MAX_ATTEMPTS=60; SLEEP_SECONDS=5; attempt=0; while ! redis-cli -h hyperswitch-v1-redis-master -p 6379 ping; do
              if [ $attempt -ge $MAX_ATTEMPTS ]; then
                echo "Redis did not become ready in time";
                exit 1;
              fi;
              attempt=$((attempt+1));
              echo "Waiting for Redis to be ready... Attempt: $attempt";
              sleep $SLEEP_SECONDS;
            done; echo "Redis is ready.";
          command:
          - /bin/sh
          - -c
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          name: check-redis
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-router-role
        serviceAccountName: hyperswitch-router-role
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: router-cm-hyperswitch-v1
          name: hyperswitch-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: grafana
      pod-template-hash: 5c95d79b6
    name: hyperswitch-v1-grafana-5c95d79b6
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hyperswitch-v1-grafana
      uid: ea5853a5-af48-41e7-88c9-ace5c94d937c
    resourceVersion: "12673"
    uid: 477eb684-2703-4c24-841f-00f88e117d78
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: grafana
        pod-template-hash: 5c95d79b6
    template:
      metadata:
        annotations:
          checksum/config: 322df4b327f90632afc999b8103eaa8db5c5b439456313837a9f8e936616cfec
          checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
          checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
          checksum/secret: 41fa47582aada96cfd80f98d18f43725edec86c8243f67111c5621507e907350
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: grafana
          pod-template-hash: 5c95d79b6
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: hyperswitch-v1-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: hyperswitch-v1-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.19.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: hyperswitch-v1-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: hyperswitch-v1-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: grafana/grafana:10.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsUser: 472
        serviceAccount: hyperswitch-v1-grafana
        serviceAccountName: hyperswitch-v1-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: hyperswitch-v1-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: hyperswitch-web
      app.kubernetes.io/version: 0.15.8
      helm.sh/chart: hyperswitch-web-0.2.0
      pod-template-hash: 5475d55478
    name: hyperswitch-web-5475d55478
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hyperswitch-web
      uid: 53c7a49f-bc97-4d2b-bc6b-c3daaf868e09
    resourceVersion: "12466"
    uid: 647ff65c-b047-4888-bddd-ed6183d22ebb
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: hyperswitch-web
        pod-template-hash: 5475d55478
    template:
      metadata:
        annotations:
          helm.sh/hook: post-install,post-upgrade
          helm.sh/hook-delete-policy: hook-succeeded
          helm.sh/hook-weight: "-5"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: hyperswitch-web
          app.kubernetes.io/version: 0.15.8
          helm.sh/chart: hyperswitch-web-0.2.0
          pod-template-hash: 5475d55478
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: hyperswitch-web-nginx
          image: juspaydotin/hyperswitch-web:0.103.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /web/0.103.1/v0/
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: hyperswitch-web
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /web/0.103.1/v0/
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 1500m
              memory: 3Gi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/nginx/conf.d/default.conf
            name: nginx-config-volume
            subPath: default.conf
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-web
        serviceAccountName: hyperswitch-web
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: hyperswitch-web-nginx
          name: nginx-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/name: mailhog
      pod-template-hash: 765485df76
    name: mailhog-765485df76
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mailhog
      uid: a54e03b9-882c-48ae-9bc7-f38a98fc1ace
    resourceVersion: "12474"
    uid: 8517d8d2-6a65-432d-b9ee-0b270a196e74
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: mailhog
        pod-template-hash: 765485df76
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: mailhog
          pod-template-hash: 765485df76
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: MH_HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: docker.io/mailhog/mailhog:v1.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: tcp-smtp
            timeoutSeconds: 1
          name: mailhog
          ports:
          - containerPort: 8025
            name: http
            protocol: TCP
          - containerPort: 1025
            name: tcp-smtp
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: tcp-smtp
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: mailhog
        serviceAccountName: mailhog
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
      pod-template-hash: 8ff4967cb
    name: prometheus-kube-state-metrics-8ff4967cb
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: 41c943d7-6109-4dd7-ac6d-e838bf7defc8
    resourceVersion: "12677"
    uid: 3335467a-7012-47d0-8e02-42d59edb46b6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 8ff4967cb
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.0
          pod-template-hash: 8ff4967cb
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.10.0
      helm.sh/chart: prometheus-pushgateway-2.16.0
      pod-template-hash: 7d7b6497cd
    name: prometheus-prometheus-pushgateway-7d7b6497cd
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: 518d3c97-33e1-4643-9a1f-f07aa1dd9082
    resourceVersion: "12734"
    uid: 981edccf-70d4-4035-9b85-ecf24d3d3430
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 7d7b6497cd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.10.0
          helm.sh/chart: prometheus-pushgateway-2.16.0
          pod-template-hash: 7d7b6497cd
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.10.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.1.0
      pod-template-hash: 795bb7fcb4
    name: prometheus-server-795bb7fcb4
    namespace: hyperswitch
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 537d3431-3699-4471-bd9e-72ac0bb4cf34
    resourceVersion: "12780"
    uid: 5d2aeaa6-93cf-441a-b85a-e0a5cd6b4d5a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 795bb7fcb4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.1.0
          pod-template-hash: 795bb7fcb4
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.78.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/version: 24.10.2
      helm.sh/chart: clickhouse-6.3.3
      shard: "0"
    name: clickhouse-shard0
    namespace: hyperswitch
    resourceVersion: "12744"
    uid: e2a72ffa-ba4d-46c6-9a9d-221b5bca0d5e
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: clickhouse
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: clickhouse
    serviceName: clickhouse-headless
    template:
      metadata:
        annotations:
          checksum/config: 93ab53427cc68197b370234298563c92471a283e62e59d6048000a87cd546bd1
          checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
          checksum/config-users-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: clickhouse
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: clickhouse
          app.kubernetes.io/version: 24.10.2
          helm.sh/chart: clickhouse-6.3.3
          shard: "0"
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: clickhouse
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: clickhouse
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - command:
          - /scripts/setup.sh
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: CLICKHOUSE_HTTP_PORT
            value: "8123"
          - name: CLICKHOUSE_TCP_PORT
            value: "9000"
          - name: CLICKHOUSE_MYSQL_PORT
            value: "9004"
          - name: CLICKHOUSE_POSTGRESQL_PORT
            value: "9005"
          - name: CLICKHOUSE_INTERSERVER_HTTP_PORT
            value: "9009"
          - name: CLICKHOUSE_ADMIN_USER
            value: default
          - name: CLICKHOUSE_SHARD_ID
            value: shard0
          - name: CLICKHOUSE_REPLICA_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: CLICKHOUSE_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: clickhouse
          - name: KEEPER_NODE_0
            value: hyperswitch-v1-zookeeper-0.hyperswitch-v1-zookeeper-headless.hyperswitch.svc.cluster.local
          image: docker.io/bitnami/clickhouse:24.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: clickhouse
          ports:
          - containerPort: 8123
            name: http
            protocol: TCP
          - containerPort: 9000
            name: tcp
            protocol: TCP
          - containerPort: 9005
            name: tcp-postgresql
            protocol: TCP
          - containerPort: 9004
            name: tcp-mysql
            protocol: TCP
          - containerPort: 9009
            name: http-intersrv
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/clickhouse/etc
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/clickhouse/logs
            name: empty-dir
            subPath: app-logs-dir
          - mountPath: /opt/bitnami/clickhouse/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /scripts/setup.sh
            name: scripts
            subPath: setup.sh
          - mountPath: /bitnami/clickhouse
            name: data
          - mountPath: /bitnami/clickhouse/etc/conf.d/default
            name: config
          - mountPath: /docker-entrypoint-initdb.d
            name: initdb-scripts
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - git clone --depth 1 --branch 2024.12.19.0 https://github.com/juspay/hyperswitch.git
            /scripts && cp /scripts/crates/analytics/docs/clickhouse/scripts/*.sql
            /docker-entrypoint-initdb.d/
          image: alpine/git
          imagePullPolicy: Always
          name: clone-sql-scripts
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /docker-entrypoint-initdb.d
            name: initdb-scripts
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: clickhouse
        serviceAccountName: clickhouse
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: clickhouse-scripts
          name: scripts
        - emptyDir: {}
          name: empty-dir
        - configMap:
            defaultMode: 420
            name: clickhouse
          name: config
        - emptyDir: {}
          name: initdb-scripts
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: clickhouse
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: clickhouse
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: clickhouse-shard0-8459b67cd6
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: clickhouse-shard0-8459b67cd6
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: locker-db
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-locker-db
    namespace: hyperswitch
    resourceVersion: "12686"
    uid: f0795c6e-e08d-4cb0-b31a-f8259d84f897
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: locker-db
    serviceName: hyperswitch-v1-locker-db-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: locker-db
          app.kubernetes.io/version: 16.1.0
          helm.sh/chart: postgresql-13.2.27
        name: hyperswitch-v1-locker-db
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: locker-db
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: db_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-locker-db
          - name: POSTGRES_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: postgres-password
                name: hyperswitch-v1-locker-db
          - name: POSTGRES_DATABASE
            value: locker-db
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "db_user" -d "dbname=locker-db" -h 127.0.0.1 -p
                5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "db_user" -d "dbname=locker-db" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: hyperswitch-v1-locker-db-8444b58bbf
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: hyperswitch-v1-locker-db-8444b58bbf
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-postgresql
    namespace: hyperswitch
    resourceVersion: "12712"
    uid: 2f18f424-980d-46de-8499-f03b0a7e1d0a
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: postgresql
    serviceName: hyperswitch-v1-postgresql-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 16.1.0
          helm.sh/chart: postgresql-13.2.27
        name: hyperswitch-v1-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: hyperswitch
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: POSTGRES_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: postgres-password
                name: hyperswitch-v1-postgresql
          - name: POSTGRES_DATABASE
            value: hyperswitch
          - name: POSTGRES_REPLICATION_MODE
            value: master
          - name: POSTGRES_REPLICATION_USER
            value: repl_user
          - name: POSTGRES_REPLICATION_PASSWORD
            valueFrom:
              secretKeyRef:
                key: replication-password
                name: hyperswitch-v1-postgresql
          - name: POSTGRES_CLUSTER_APP_NAME
            value: my_application
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "hyperswitch" -d "dbname=hyperswitch" -h 127.0.0.1
                -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "hyperswitch" -d "dbname=hyperswitch" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 150m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: hyperswitch-v1-postgresql-64ccf8b5f
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: hyperswitch-v1-postgresql-64ccf8b5f
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: read
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 16.1.0
      helm.sh/chart: postgresql-13.2.27
    name: hyperswitch-v1-postgresql-read
    namespace: hyperswitch
    resourceVersion: "922"
    uid: e6df9632-8c05-47ee-80fa-92972e1975c5
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: read
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: postgresql
    serviceName: hyperswitch-v1-postgresql-read-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: read
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 16.1.0
          helm.sh/chart: postgresql-13.2.27
        name: hyperswitch-v1-postgresql-read
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: read
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: hyperswitch
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: hyperswitch-v1-postgresql
          - name: POSTGRES_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: postgres-password
                name: hyperswitch-v1-postgresql
          - name: POSTGRES_REPLICATION_MODE
            value: slave
          - name: POSTGRES_REPLICATION_USER
            value: repl_user
          - name: POSTGRES_REPLICATION_PASSWORD
            valueFrom:
              secretKeyRef:
                key: replication-password
                name: hyperswitch-v1-postgresql
          - name: POSTGRES_CLUSTER_APP_NAME
            value: my_application
          - name: POSTGRES_MASTER_HOST
            value: hyperswitch-v1-postgresql
          - name: POSTGRES_MASTER_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:16.1.0-debian-11-r18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "hyperswitch" -d "dbname=hyperswitch" -h 127.0.0.1
                -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "hyperswitch" -d "dbname=hyperswitch" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentRevision: hyperswitch-v1-postgresql-read-6fbd45b4fb
    observedGeneration: 1
    replicas: 0
    updateRevision: hyperswitch-v1-postgresql-read-6fbd45b4fb
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: master
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.3
      helm.sh/chart: redis-18.6.1
    name: hyperswitch-v1-redis-master
    namespace: hyperswitch
    resourceVersion: "12751"
    uid: 818258e4-f59a-40e8-8b25-52ba6d259a1e
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: master
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: redis
    serviceName: hyperswitch-v1-redis-headless
    template:
      metadata:
        annotations:
          checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
          checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
          checksum/scripts: 1a2a98d8e7da56e19dce6d5c5eff0a4b986298ec80143cde0afb2aa6a02d9db8
          checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: master
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: redis
          app.kubernetes.io/version: 7.2.3
          helm.sh/chart: redis-18.6.1
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: master
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: true
        containers:
        - args:
          - -c
          - /opt/bitnami/scripts/start-scripts/start-master.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: REDIS_REPLICATION_MODE
            value: master
          - name: ALLOW_EMPTY_PASSWORD
            value: "yes"
          - name: REDIS_TLS_ENABLED
            value: "no"
          - name: REDIS_PORT
            value: "6379"
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_liveness_local.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_readiness_local.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/scripts/start-scripts
            name: start-scripts
          - mountPath: /health
            name: health
          - mountPath: /data
            name: redis-data
          - mountPath: /opt/bitnami/redis/mounted-etc
            name: config
          - mountPath: /opt/bitnami/redis/etc/
            name: redis-tmp-conf
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: hyperswitch-v1-redis
        serviceAccountName: hyperswitch-v1-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: hyperswitch-v1-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: hyperswitch-v1-redis-health
          name: health
        - configMap:
            defaultMode: 420
            name: hyperswitch-v1-redis-configuration
          name: config
        - emptyDir: {}
          name: redis-tmp-conf
        - emptyDir: {}
          name: tmp
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: master
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: redis
        name: redis-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: hyperswitch-v1-redis-master-5678dcb954
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: hyperswitch-v1-redis-master-5678dcb954
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: replica
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis
      app.kubernetes.io/version: 7.2.3
      helm.sh/chart: redis-18.6.1
    name: hyperswitch-v1-redis-replicas
    namespace: hyperswitch
    resourceVersion: "967"
    uid: ed5e2567-0107-4d02-90d0-f1437aa214aa
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: replica
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: redis
    serviceName: hyperswitch-v1-redis-headless
    template:
      metadata:
        annotations:
          checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
          checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
          checksum/scripts: 1a2a98d8e7da56e19dce6d5c5eff0a4b986298ec80143cde0afb2aa6a02d9db8
          checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: replica
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: redis
          app.kubernetes.io/version: 7.2.3
          helm.sh/chart: redis-18.6.1
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: replica
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: redis
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: true
        containers:
        - args:
          - -c
          - /opt/bitnami/scripts/start-scripts/start-replica.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: REDIS_REPLICATION_MODE
            value: replica
          - name: REDIS_MASTER_HOST
            value: hyperswitch-v1-redis-master-0.hyperswitch-v1-redis-headless.hyperswitch.svc.cluster.local
          - name: REDIS_MASTER_PORT_NUMBER
            value: "6379"
          - name: ALLOW_EMPTY_PASSWORD
            value: "yes"
          - name: REDIS_TLS_ENABLED
            value: "no"
          - name: REDIS_PORT
            value: "6379"
          image: docker.io/bitnami/redis:7.2.3-debian-11-r2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_liveness_local_and_master.sh 5
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 6
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - /health/ping_readiness_local_and_master.sh 1
            failureThreshold: 5
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: redis
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/scripts/start-scripts
            name: start-scripts
          - mountPath: /health
            name: health
          - mountPath: /data
            name: redis-data
          - mountPath: /opt/bitnami/redis/mounted-etc
            name: config
          - mountPath: /opt/bitnami/redis/etc
            name: redis-tmp-conf
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: hyperswitch-v1-redis
        serviceAccountName: hyperswitch-v1-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 493
            name: hyperswitch-v1-redis-scripts
          name: start-scripts
        - configMap:
            defaultMode: 493
            name: hyperswitch-v1-redis-health
          name: health
        - configMap:
            defaultMode: 420
            name: hyperswitch-v1-redis-configuration
          name: config
        - emptyDir: {}
          name: redis-tmp-conf
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: replica
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: redis
        name: redis-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 0
    collisionCount: 0
    currentRevision: hyperswitch-v1-redis-replicas-586659d584
    observedGeneration: 1
    replicas: 0
    updateRevision: hyperswitch-v1-redis-replicas-586659d584
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: Aggregator
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: vector
      app.kubernetes.io/version: 0.42.0-distroless-libc
      helm.sh/chart: vector-0.37.0
    name: hyperswitch-v1-vector
    namespace: hyperswitch
    resourceVersion: "12650"
    uid: e48b8604-6351-4076-9653-18b85b857e5c
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: Aggregator
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: vector
    serviceName: hyperswitch-v1-vector-headless
    template:
      metadata:
        annotations:
          checksum/config: 2b39e38c4ca14a82b11b27b4fa71b9062af3e91bada795fe8fc4fcb190cd21c1
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: Aggregator
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/name: vector
          vector.dev/exclude: "true"
      spec:
        containers:
        - args:
          - --config-dir
          - /etc/vector/
          env:
          - name: VECTOR_LOG
            value: info
          - name: KAFKA_HOST
            value: kafka0:29092
          image: timberio/vector:0.42.0-distroless-libc
          imagePullPolicy: IfNotPresent
          name: vector
          ports:
          - containerPort: 8686
            name: api
            protocol: TCP
          - containerPort: 3103
            name: sdk-source
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /vector-data-dir
            name: data
          - mountPath: /etc/vector/
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hyperswitch-v1-vector
        serviceAccountName: hyperswitch-v1-vector
        terminationGracePeriodSeconds: 60
        volumes:
        - emptyDir: {}
          name: data
        - name: config
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: hyperswitch-v1-vector
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: hyperswitch-v1-vector-f87d7b4bd
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: hyperswitch-v1-vector-f87d7b4bd
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: zookeeper
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/version: 3.9.3
      helm.sh/chart: zookeeper-13.6.0
      role: zookeeper
    name: hyperswitch-v1-zookeeper
    namespace: hyperswitch
    resourceVersion: "12708"
    uid: 839bd331-8c9f-4d70-8c52-876c7316b483
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: zookeeper
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: zookeeper
    serviceName: hyperswitch-v1-zookeeper-headless
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: zookeeper
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: zookeeper
          app.kubernetes.io/version: 3.9.3
          helm.sh/chart: zookeeper-13.6.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: zookeeper
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - command:
          - /scripts/setup.sh
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: ZOO_DATA_LOG_DIR
          - name: ZOO_PORT_NUMBER
            value: "2181"
          - name: ZOO_TICK_TIME
            value: "2000"
          - name: ZOO_INIT_LIMIT
            value: "10"
          - name: ZOO_SYNC_LIMIT
            value: "5"
          - name: ZOO_PRE_ALLOC_SIZE
            value: "65536"
          - name: ZOO_SNAPCOUNT
            value: "100000"
          - name: ZOO_MAX_CLIENT_CNXNS
            value: "60"
          - name: ZOO_4LW_COMMANDS_WHITELIST
            value: srvr, mntr, ruok
          - name: ZOO_LISTEN_ALLIPS_ENABLED
            value: "no"
          - name: ZOO_AUTOPURGE_INTERVAL
            value: "1"
          - name: ZOO_AUTOPURGE_RETAIN_COUNT
            value: "10"
          - name: ZOO_MAX_SESSION_TIMEOUT
            value: "40000"
          - name: ZOO_SERVERS
            value: hyperswitch-v1-zookeeper-0.hyperswitch-v1-zookeeper-headless.hyperswitch.svc.cluster.local:2888:3888::1
          - name: ZOO_ENABLE_AUTH
            value: "no"
          - name: ZOO_ENABLE_QUORUM_AUTH
            value: "no"
          - name: ZOO_HEAP_SIZE
            value: "1024"
          - name: ZOO_LOG_LEVEL
            value: ERROR
          - name: ALLOW_ANONYMOUS_LOGIN
            value: "yes"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: ZOO_ADMIN_SERVER_PORT_NUMBER
            value: "8080"
          image: docker.io/bitnami/zookeeper:3.8.4-debian-12-r16
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/bash
              - -ec
              - ZOO_HC_TIMEOUT=3 /opt/bitnami/scripts/zookeeper/healthcheck.sh
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: zookeeper
          ports:
          - containerPort: 2181
            name: client
            protocol: TCP
          - containerPort: 8080
            name: http-admin
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/bash
              - -ec
              - ZOO_HC_TIMEOUT=2 /opt/bitnami/scripts/zookeeper/healthcheck.sh
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 2Gi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/zookeeper/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/zookeeper/logs
            name: empty-dir
            subPath: app-logs-dir
          - mountPath: /scripts/setup.sh
            name: scripts
            subPath: setup.sh
          - mountPath: /bitnami/zookeeper
            name: data
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: hyperswitch-v1-zookeeper
        serviceAccountName: hyperswitch-v1-zookeeper
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            defaultMode: 493
            name: hyperswitch-v1-zookeeper-scripts
          name: scripts
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: hyperswitch-v1-zookeeper-65dbb579dc
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: hyperswitch-v1-zookeeper-65dbb579dc
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: broker
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.9.0
      helm.sh/chart: kafka-31.0.0
    name: kafka0-broker
    namespace: hyperswitch
    resourceVersion: "12695"
    uid: e715a825-f9c7-44bf-8c04-941678350f01
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: broker
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: kafka
        app.kubernetes.io/part-of: kafka
    serviceName: kafka0-broker-headless
    template:
      metadata:
        annotations:
          checksum/configuration: e2338a6a5df0ed29e2a86beac8a45e32a6d5bd3c0f06f968c0112522df877db7
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: broker
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kafka
          app.kubernetes.io/part-of: kafka
          app.kubernetes.io/version: 3.9.0
          helm.sh/chart: kafka-31.0.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: broker
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: kafka
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: KAFKA_HEAP_OPTS
            value: -Xmx1024m -Xms1024m
          - name: KAFKA_KRAFT_CLUSTER_ID
            valueFrom:
              secretKeyRef:
                key: kraft-cluster-id
                name: kafka0-kraft-cluster-id
          image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - pgrep
              - -f
              - kafka
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kafka
          ports:
          - containerPort: 9092
            name: client
            protocol: TCP
          - containerPort: 9094
            name: interbroker
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: client
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/kafka
            name: data
          - mountPath: /opt/bitnami/kafka/logs
            name: logs
          - mountPath: /opt/bitnami/kafka/config/server.properties
            name: kafka-config
            subPath: server.properties
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - args:
          - -ec
          - |
            /scripts/kafka-init.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: KAFKA_VOLUME_DIR
            value: /bitnami/kafka
          - name: KAFKA_MIN_ID
            value: "100"
          image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
          imagePullPolicy: IfNotPresent
          name: kafka-init
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/kafka
            name: data
          - mountPath: /config
            name: kafka-config
          - mountPath: /configmaps
            name: kafka-configmaps
          - mountPath: /secret-config
            name: kafka-secret-config
          - mountPath: /scripts
            name: scripts
          - mountPath: /tmp
            name: tmp
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kafka0
        serviceAccountName: kafka0
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kafka0-broker-configuration
          name: kafka-configmaps
        - emptyDir: {}
          name: kafka-secret-config
        - emptyDir: {}
          name: kafka-config
        - emptyDir: {}
          name: tmp
        - configMap:
            defaultMode: 493
            name: kafka0-scripts
          name: scripts
        - emptyDir: {}
          name: logs
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: kafka0-broker-58584c8697
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: kafka0-broker-58584c8697
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/instance: hyperswitch-v1
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: kafka
      app.kubernetes.io/version: 3.9.0
      helm.sh/chart: kafka-31.0.0
    name: kafka0-controller
    namespace: hyperswitch
    resourceVersion: "12700"
    uid: 66686a07-fdb5-4429-8203-e3b27b1a4883
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller-eligible
        app.kubernetes.io/instance: hyperswitch-v1
        app.kubernetes.io/name: kafka
        app.kubernetes.io/part-of: kafka
    serviceName: kafka0-controller-headless
    template:
      metadata:
        annotations:
          checksum/configuration: 42fd96e136d37d8e7074e5c26e823e42038d92b27c338ecd3de1b915d0da2f94
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller-eligible
          app.kubernetes.io/instance: hyperswitch-v1
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kafka
          app.kubernetes.io/part-of: kafka
          app.kubernetes.io/version: 3.9.0
          helm.sh/chart: kafka-31.0.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: controller-eligible
                    app.kubernetes.io/instance: hyperswitch-v1
                    app.kubernetes.io/name: kafka
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: KAFKA_HEAP_OPTS
            value: -Xmx1024m -Xms1024m
          - name: KAFKA_KRAFT_CLUSTER_ID
            valueFrom:
              secretKeyRef:
                key: kraft-cluster-id
                name: kafka0-kraft-cluster-id
          image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - pgrep
              - -f
              - kafka
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kafka
          ports:
          - containerPort: 9093
            name: controller
            protocol: TCP
          - containerPort: 9092
            name: client
            protocol: TCP
          - containerPort: 9094
            name: interbroker
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: controller
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/kafka
            name: data
          - mountPath: /opt/bitnami/kafka/logs
            name: logs
          - mountPath: /opt/bitnami/kafka/config/server.properties
            name: kafka-config
            subPath: server.properties
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - args:
          - -ec
          - |
            /scripts/kafka-init.sh
          command:
          - /bin/bash
          env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: KAFKA_VOLUME_DIR
            value: /bitnami/kafka
          - name: KAFKA_MIN_ID
            value: "0"
          image: docker.io/bitnami/kafka:3.9.0-debian-12-r1
          imagePullPolicy: IfNotPresent
          name: kafka-init
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /bitnami/kafka
            name: data
          - mountPath: /config
            name: kafka-config
          - mountPath: /configmaps
            name: kafka-configmaps
          - mountPath: /secret-config
            name: kafka-secret-config
          - mountPath: /scripts
            name: scripts
          - mountPath: /tmp
            name: tmp
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kafka0
        serviceAccountName: kafka0
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kafka0-controller-configuration
          name: kafka-configmaps
        - emptyDir: {}
          name: kafka-secret-config
        - emptyDir: {}
          name: kafka-config
        - emptyDir: {}
          name: tmp
        - configMap:
            defaultMode: 493
            name: kafka0-scripts
          name: scripts
        - emptyDir: {}
          name: logs
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: kafka0-controller-745f5b8ccf
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: kafka0-controller-745f5b8ccf
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: hyperswitch-v1
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-16T10:16:05Z"
    generation: 1
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: hyperswitch-v1
    name: loki
    namespace: hyperswitch
    resourceVersion: "12816"
    uid: 43a7d275-7730-4efb-9e22-4f809c52e953
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: loki
        release: hyperswitch-v1
    serviceName: loki-headless
    template:
      metadata:
        annotations:
          checksum/config: 710cdc54bd9200910bfcca95c22c8a53056e7b37cd3ac7be301da54dbee52dc9
          prometheus.io/port: http-metrics
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: loki
          name: loki
          release: hyperswitch-v1
      spec:
        affinity: {}
        containers:
        - args:
          - -config.file=/etc/loki/loki.yaml
          image: grafana/loki:2.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: memberlist-port
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /etc/loki
            name: config
          - mountPath: /data
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 4800
        volumes:
        - emptyDir: {}
          name: tmp
        - name: config
          secret:
            defaultMode: 420
            secretName: loki
        - emptyDir: {}
          name: storage
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: loki-564bc8cc5b
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: loki-564bc8cc5b
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: hyperswitch
    creationTimestamp: "2025-01-19T18:44:31Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.27.0
      helm.sh/chart: alertmanager-1.13.1
    name: prometheus-alertmanager
    namespace: hyperswitch
    resourceVersion: "12574"
    uid: 0d13dc8f-ddda-4dc7-82c9-ad08f5c52528
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: alertmanager
    serviceName: prometheus-alertmanager-headless
    template:
      metadata:
        annotations:
          checksum/config: db42e6af1a032cfdcfd2bad943a75a924fe77785daf0a778e75e929dae1093c7
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: alertmanager
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --storage.path=/alertmanager
          - --config.file=/etc/alertmanager/alertmanager.yml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.27.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/alertmanager
            name: config
          - mountPath: /alertmanager
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-alertmanager
        serviceAccountName: prometheus-alertmanager
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-alertmanager
          name: config
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-alertmanager-6565764849
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-alertmanager-6565764849
    updatedReplicas: 1
kind: List
metadata:
  resourceVersion: ""
